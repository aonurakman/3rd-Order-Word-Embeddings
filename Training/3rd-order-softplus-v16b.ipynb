{"cells":[{"cell_type":"markdown","source":["W/o .neg()\n","\n","HAD TO DELETE MINUS, because loss was negative"],"metadata":{"id":"8LbG1WO4h-oD"}},{"cell_type":"code","source":["version = \"v16b\""],"metadata":{"id":"1qmG0753YLdj","executionInfo":{"status":"ok","timestamp":1688990208489,"user_tz":-120,"elapsed":253,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["################ For subsampling ################\n","# Remove words (tokens) for frequent occurances?\n","enable_subsampling = True\n","# Lower threshold -> More removal\n","subsample_threshold = 1e-5\n","\n","\n","# Remove words (types) with low context representation?\n","remove_low_context = False\n","# Higher threshold -> More removal\n","low_context_threshold = 100\n","\n","################ Model parameters ################\n","# Size of embeddings\n","embedding_dim = 300\n","# -1 for dynamic on context size, pick minimum 2\n","win_size = 4\n","# Number of neg samples per one context word\n","neg_samp = 5\n","\n","\n","################ Training parameters ################\n","# Size ratio val/total\n","val_partition = 0.1\n","\n","# Higher -> Faster, Lower -> More generalization\n","batch_size = 128\n","# Batch for val loss calculation\n","validation_batch_size = 1024\n","\n","# Length of training\n","epochs = 3\n","\n","# Step size in learning\n","learning_rate = 0.003\n","# 1.0 -> No decay, (0 , 1.0) -> LR drops in process\n","lr_decay_rate = 1.0\n","# Update LR per how many epochs\n","lr_decay_step = 1"],"metadata":{"id":"WQGVBARxkRSD","executionInfo":{"status":"ok","timestamp":1688990208989,"user_tz":-120,"elapsed":7,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9bMbdtSfM6lL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688990210818,"user_tz":-120,"elapsed":1835,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"47c20711-b529-43f2-ec83-e856adaa1dbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Experiments/3rd-order-softplus\n","data  models  standards  voca\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Experiments/3rd-order-softplus\n","!ls"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"SlDIJcLwz-nr","executionInfo":{"status":"ok","timestamp":1688990216156,"user_tz":-120,"elapsed":5342,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["import copy\n","import csv\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pickle\n","import random\n","import re\n","import time\n","import torch\n","from collections import Counter\n","from prettytable import PrettyTable\n","from scipy.stats import spearmanr\n","from statistics import mean\n","from torch import nn\n","import torch.optim as optim"]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Running on '%s'.\" % (device))"],"metadata":{"id":"pYWjOaGbH26S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688990216157,"user_tz":-120,"elapsed":24,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"e27860c1-5675-4b48-82fe-2fa1cb271b93"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on 'cuda'.\n"]}]},{"cell_type":"code","source":["try:\n","  os.makedirs('voca/' + version)\n","  print(\"'voca/\" + version + \"' created.\")\n","except OSError as error:\n","  print(\"'voca/\" + version + \"' already exists.\")\n","\n","try:\n","  os.makedirs('models/' + version)\n","  print(\"'models/\" + version + \"' created.\")\n","except OSError as error:\n","  print(\"'models/\" + version + \"' already exists.\")\n"],"metadata":{"id":"mj81fjZzcytn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688990216158,"user_tz":-120,"elapsed":19,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"19fc8de3-d62c-4df1-ade4-f1b97ef546f4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["'voca/v16b' already exists.\n","'models/v16b' already exists.\n"]}]},{"cell_type":"code","source":["to_path = lambda s1, s2, s3: s1 + \"/\" + s2 + \"/\" + s3\n","to_tensor = lambda v: torch.LongTensor(v).to(device)"],"metadata":{"id":"XU55_M8zCop0","executionInfo":{"status":"ok","timestamp":1688990216158,"user_tz":-120,"elapsed":16,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Prepare Vocab"],"metadata":{"id":"H3EQLW1royGR"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"qvbjfp1X0IXK","executionInfo":{"status":"ok","timestamp":1688990216158,"user_tz":-120,"elapsed":16,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def preprocess(text):\n","\n","  # lowercase\n","  text = text.lower()\n","\n","  # separate these punctuation marks from the words in the text and allow the model to treat them differently.\n","  text = text.replace('.', ' <PERIOD> ')\n","  text = text.replace(',', ' <COMMA> ')\n","  text = text.replace('\"', ' <QUOTATION_MARK> ')\n","  text = text.replace(';', ' <SEMICOLON> ')\n","  text = text.replace('!', ' <EXCLAMATION_MARK> ')\n","  text = text.replace('?', ' <QUESTION_MARK> ')\n","  text = text.replace('(', ' <LEFT_PAREN> ')\n","  text = text.replace(')', ' <RIGHT_PAREN> ')\n","  text = text.replace('--', ' <HYPHENS> ')\n","  text = text.replace('?', ' <QUESTION_MARK> ')\n","  text = text.replace('\\n', ' <NEW_LINE> ')\n","  text = text.replace(':', ' <COLON> ')\n","\n","  # splits the text into individual words\n","  words = text.split()\n","\n","  # remove words with five or fewer occurrences. reduce the size of the vocabulary and improve the efficiency of the model.\n","  word_counts = Counter(words)  # dictionary word:occurrences\n","  trimmed_words = [word for word in words if word_counts[word] > 5] # Remove any word fewer than 5 tokens\n","\n","  # remove common words that do not provide much meaning to the text\n","  stop = [\n","  \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"also\", \"altough\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\",\n","  \"b\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\",\n","  \"c\", \"can\", \"can't\", \"cannot\", \"could\", \"couldn't\",\n","  \"d\", \"de\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\",\n","  \"e\", \"each\", \"either\", \"even\",\n","  \"f\", \"few\", \"for\", \"from\", \"further\",\n","  \"g\",\n","  \"h\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\",\n","  \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"however\",\n","  \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"ii\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\",\n","  \"j\", \"just\",\n","  \"k\",\n","  \"l\", \"like\",\n","  \"m\", \"many\", \"may\", \"me\", \"more\", \"most\", \"much\", \"must\", \"my\", \"myself\",\n","  \"n\", \"nd\", \"neither\", \"no\", \"nor\", \"not\", \"now\",\n","  \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n","  \"p\",\n","  \"q\",\n","  \"r\", \"rd\",\n","  \"s\", \"same\", \"shall\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\",\n","  \"t\", \"th\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\",\n","  \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"though\", \"through\", \"to\", \"too\",\n","  \"u\", \"under\", \"until\", \"up\", \"us\",\n","  \"v\", \"very\",\n","  \"w\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\",\n","  \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"will\", \"with\", \"won't\", \"would\", \"wouldn't\",\n","  \"x\",\n","  \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n","  \"z\",\n","  \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\"\n","  ]\n","\n","  stop_trimmed_words = [w for w in trimmed_words if w not in stop]\n","\n","  return stop_trimmed_words"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"4U-wHrzq0NR-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688990262345,"user_tz":-120,"elapsed":46202,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"8639df5b-1d93-4f93-d0b2-4c2d63a1d371"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total words in text: 8448361\n","Unique words in text: 63459\n"]}],"source":["with open('data/text8') as f:\n","    text = f.read()\n","\n","words = preprocess(text)\n","\n","print(\"Total words in text: %d\" % (len(words)))\n","print(\"Unique words in text: %d\" % (len(set(words))))"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"EHRKM-rQ0LR-","executionInfo":{"status":"ok","timestamp":1688990262346,"user_tz":-120,"elapsed":16,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def create_lookup_tables(words):\n","\n","  word_counts = Counter(words)  # dictionary \"word:number of occurrences\"\n","\n","  count = []  # list of tuples (word, number of occurrences) from most frequent, e.g. [('one', 411764),...]\n","  count.extend(Counter(words).most_common(len(words)))\n","\n","  sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True) # list of words sorted in decreasing frequency\n","  # order of a word in this list becomes its index\n","\n","  int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}  # dictionary index:word {0: 'one',...}\n","  vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}  # dictionary word:index {'one': 0,...}\n","\n","  return vocab_to_int, int_to_vocab, count"]},{"cell_type":"code","source":["vocab_to_int, int_to_vocab, count = create_lookup_tables(words)"],"metadata":{"id":"ZbaXIpEeu774","executionInfo":{"status":"ok","timestamp":1688990264895,"user_tz":-120,"elapsed":1735,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def prepare_train_words(words, enable_subsampling, subsample_threshold):\n","\n","  train_words = []\n","\n","  if enable_subsampling:\n","\n","    int_words = [vocab_to_int[word] for word in words]\n","    word_counts = Counter(int_words)\n","\n","    freqs = {word: count / len(int_words) for word, count in word_counts.items()} # dict word_idx:rel_freq\n","    p_drop = {word: 1 - np.sqrt(subsample_threshold / freqs[word]) for word in word_counts} # dict word_idx:prob_dropping\n","\n","    train_words = [word for word in int_words if random.random() < (1 - p_drop[word])] # list of words after subsampling\n","\n","  else:\n","\n","    train_words = [vocab_to_int[word] for word in words]\n","\n","  return train_words"],"metadata":{"id":"aXii03BCuI-A","executionInfo":{"status":"ok","timestamp":1688990264895,"user_tz":-120,"elapsed":4,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"nQhfgkmz5BNt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688990270160,"user_tz":-120,"elapsed":5268,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"af810f07-fce1-44b7-81e8-092b11a3e45e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total words for training: 3427226\n","Unique words for training: 63459\n"]}],"source":["train_words = prepare_train_words(words, enable_subsampling, subsample_threshold)\n","\n","print(\"Total words for training: {}\".format(len(train_words)))\n","print(\"Unique words for training: {}\".format(len(set(train_words))))"]},{"cell_type":"markdown","source":["# Save vocab to file"],"metadata":{"id":"5QVF85twvZmk"}},{"cell_type":"code","execution_count":17,"metadata":{"id":"yuk6LYHi0WOs","executionInfo":{"status":"ok","timestamp":1688990270160,"user_tz":-120,"elapsed":4,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def save_vocab(count, vocab_words):\n","  with open(to_path(\"voca\", version, \"voca.txt\"), \"w\") as f:\n","    for i in range(len(count)):\n","      vocab_word = vocab_words[i]\n","      f.write(\"%s %d\\n\" % (vocab_word, count[i][1]))"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Eg9x1w9z0X7n","executionInfo":{"status":"ok","timestamp":1688990270807,"user_tz":-120,"elapsed":650,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["save_vocab(count, int_to_vocab)"]},{"cell_type":"markdown","source":["# Sampling"],"metadata":{"id":"8_tAUy0xBhME"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"mH9QGqCs0Qck","executionInfo":{"status":"ok","timestamp":1688990270807,"user_tz":-120,"elapsed":3,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["# Create a sampling table for neg sampling, each element appears number of times relative to its frequency.\n","\n","def init_sample_table(count):\n","  count = [ele[1] for ele in count] # List of number of occurances, sorted; [occ1, occ2, occ3...]\n","\n","  pow_frequency = np.array(count) ** 0.75 # Power occurances by .75\n","  power = sum(pow_frequency) # for normalization\n","  ratio = pow_frequency / power # relative power frequencies list\n","\n","  table_size = 1e8\n","  sampling_count = np.round(ratio * table_size) # how many times an element should be added to the sampling table\n","\n","  sample_table = []\n","  for idx, x in enumerate(sampling_count):\n","    sample_table += [idx] * int(x) # add each word's idx to the table, number of times specificed in sampling_count\n","\n","  return np.array(sample_table)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"41F3kcNa0S26","executionInfo":{"status":"ok","timestamp":1688990278590,"user_tz":-120,"elapsed":7785,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["sample_table = init_sample_table(count)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"KnG8TG960Zq9","executionInfo":{"status":"ok","timestamp":1688990278590,"user_tz":-120,"elapsed":7,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def get_context(words, idx, win_size, idx_to_contexts):\n","\n","  if win_size < 0:\n","    ctx_count = len(idx_to_contexts[words[idx]])\n","    R = ( 3 - min( int( ctx_count / 150 ) , 2 ) ) * 2\n","  else:\n","    R = np.random.randint(2, max(win_size+1, 3))\n","\n","  start = idx - R if (idx - R) > 0 else 0\n","  stop = idx + R\n","  context_words = words[start : idx] + words[idx + 1 : stop + 1]\n","\n","  return list(set(context_words))"]},{"cell_type":"markdown","source":["for an index i,\n","* $central\\_words[i]$ -> a target word\n","* $context\\_words1[i]$ and $context\\_words2[i]$ -> 2 different context words, each in the context of $central\\_words[i]$\n","* $negative\\_examples[i]$ -> **list** of neg sample words to pair with the context word at $context\\_words1[i]$\n","\n","Therefore,\n","* A **positive sample**: ($central\\_words[i]$, $context\\_words1[i]$, $context\\_words2[i]$)\n","* A **negative sample**: ($central\\_words[i]$, $negative\\_examples[i][j]$, $context\\_words1[i]$)"],"metadata":{"id":"dAMWH8R5VVF1"}},{"cell_type":"code","execution_count":22,"metadata":{"id":"gWkqbb4b0hAI","executionInfo":{"status":"ok","timestamp":1688990278590,"user_tz":-120,"elapsed":4,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def get_batches(words, batch_size, win_size, neg_samp, idx_to_contexts):\n","\n","  n_batches = len(words) // batch_size\n","  words = words[:n_batches * batch_size] # keep only the number of words to match the bath_size and n_batches\n","\n","  for idx in range(0, len(words), batch_size): # for each batch in words\n","    central_words = []  # list containing target words\n","    context_words1 = []  # list of words in the context, second order\n","    context_words2 = []  # list of words in the context, third order\n","\n","    x, y = [], [] # lists that store the target and context words for each training sample\n","    z = [] # list of tuples that stores the starting and ending indices of each training sample in the x and y lists\n","    a = 0 # Counter for the total number of training samples seen so far. Used to keep track of the indices in z.\n","\n","    batch = words[idx:idx + batch_size] # one batch; batch_size number of words (unique), starting from index idx\n","\n","    for ii in range(len(batch)): # in a batch\n","      batch_x = batch[ii] # target word for a given training sample\n","      batch_y = get_context(batch, ii, win_size, idx_to_contexts) # list of context words for that target word\n","\n","      y.extend(batch_y)\n","      x.extend([batch_x] * len(batch_y)) # target word added, repeated by the number of context words\n","      z.extend([[a, len(x)]] * len(batch_y)) # a -> starting idx, len(x) -> end index (for one training sample)\n","\n","      a = a + len(batch_y)\n","\n","      central_words.extend([batch_x] * len(batch_y) * (len(batch_y) - 1)) # target word repeated by 2 comb of context words in batch\n","      for i in range(len(batch_y)): # for each context word in the batch\n","        context_words1.extend([batch_y[i]] * (len(batch_y) - 1)) # each context word, by number of context words\n","\n","    for i in range(len(z)):\n","      valori = list(range(z[i][0], z[i][1]))\n","      valori.remove(i) # context_words1[i] <> context_words2[i]\n","      for v in valori:\n","        context_words2.extend([y[v]]) # words for 3rd order, for each context word, every other context word\n","\n","    negative_examples = np.random.choice(sample_table, size=(len(central_words), neg_samp))  # list of negative samples for each context word\n","    for i in range(len(context_words1)):\n","      for j in range(neg_samp):\n","        while (context_words1[i] == negative_examples[i][j]):\n","          negative_examples[i][j] = sample_table[random.randint(0, len(sample_table))] # negative sample, make it different than context words\n","\n","    yield central_words, context_words1, context_words2, negative_examples"]},{"cell_type":"code","source":["# Get context types for words\n","idx_to_contexts = {w : set() for w in train_words}\n","\n","if win_size == -1:\n","  for input_words, target_words1, target_words2, noise_word in get_batches(train_words, 128, 2, 1, idx_to_contexts):\n","    for idx, in_w in enumerate(input_words):\n","      idx_to_contexts[in_w].add(target_words1[idx])"],"metadata":{"id":"tn8JlOMgKJGW","executionInfo":{"status":"ok","timestamp":1688990279735,"user_tz":-120,"elapsed":1148,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Removal of low context words\n","if remove_low_context:\n","  train_words = [word for word in train_words if len(idx_to_contexts[word]) > low_context_threshold]\n","  print(\"Eliminated low context words.\")\n","  print(\"Tokens for training: %d\" % (len(train_words)))\n","  print(\"Types for training: %d\" & (len(set(train_words))))"],"metadata":{"id":"Mw4JrXhxutdz","executionInfo":{"status":"ok","timestamp":1688990279735,"user_tz":-120,"elapsed":9,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Train - Validation split\n","split_index = int(len(train_words) * val_partition)\n","validation_words = train_words[:split_index]\n","train_words = train_words[split_index:]\n","\n","# Validation set should not introduce new types\n","exclude_from_val = set(validation_words) - set(train_words)\n","validation_words = [w for w in validation_words if w not in exclude_from_val]\n","\n","print(\"Tokens for training: %d\" % (len(train_words)))\n","print(\"Tokens for validation: %d\" % (len(validation_words)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JW8jDWmiKYom","executionInfo":{"status":"ok","timestamp":1688990279735,"user_tz":-120,"elapsed":8,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"37759296-f0b9-49ff-9456-a797f45bb521"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens for training: 3084504\n","Tokens for validation: 340729\n"]}]},{"cell_type":"markdown","source":["# Evaluation Functions"],"metadata":{"id":"3NOocrCko4NE"}},{"cell_type":"code","execution_count":26,"metadata":{"id":"GoR0W8400ogG","executionInfo":{"status":"ok","timestamp":1688990279736,"user_tz":-120,"elapsed":6,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def cosine_similarity(v1, v2):\n","  sumxx, sumxy, sumyy = 0, 0, 0\n","  for i in range(len(v1)):\n","    x = v1[i]\n","    y = v2[i]\n","    sumxx += x * x\n","    sumyy += y * y\n","    sumxy += x * y\n","  return sumxy / math.sqrt(sumxx * sumyy)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"60pu_ls70oa2","executionInfo":{"status":"ok","timestamp":1688990280263,"user_tz":-120,"elapsed":7,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def scorefunction1(embed, wordindex):\n","\n","  with open('standards/combined.csv') as csvfile:\n","    filein = csv.reader(csvfile)\n","    index = 0\n","    consim, humansim = [], []\n","\n","    for eles in filein:\n","\n","      if index == 0:\n","        index = 1\n","        continue\n","\n","      if (eles[0] not in wordindex) or (eles[1] not in wordindex):\n","        continue\n","\n","      word1 = int(wordindex[eles[0]])\n","      word2 = int(wordindex[eles[1]])\n","      humansim.append(float(eles[2]))\n","\n","      consim.append( cosine_similarity( embed[word1] , embed[word2] ) )\n","\n","  cor1, pvalue1 = spearmanr(humansim, consim)\n","\n","  return cor1"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Em45FUgs0oOg","executionInfo":{"status":"ok","timestamp":1688990280263,"user_tz":-120,"elapsed":6,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def scorefunction2(embed, wordindex):\n","\n","  lines = open('standards/wordsim_similarity_goldstandard.txt', 'r').readlines()\n","\n","  consim, humansim = [], []\n","\n","  for line in lines:\n","\n","    eles = line.strip().split()\n","\n","    if (eles[0] not in wordindex) or (eles[1] not in wordindex):\n","      continue\n","\n","    word1 = int(wordindex[eles[0]])\n","    word2 = int(wordindex[eles[1]])\n","    humansim.append(float(eles[2]))\n","\n","    consim.append( cosine_similarity( embed[word1] , embed[word2] ) )\n","\n","  cor2, pvalue2 = spearmanr(humansim, consim)\n","\n","  return cor2"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"5VXOX5mY0wRM","executionInfo":{"status":"ok","timestamp":1688990280263,"user_tz":-120,"elapsed":6,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["def scorefunction3(embed, wordindex):\n","\n","  lines = open('standards/wordsim_relatedness_goldstandard.txt', 'r').readlines()\n","\n","  consim, humansim = [], []\n","\n","  for line in lines:\n","\n","    eles = line.strip().split()\n","\n","    if (eles[0] not in wordindex) or (eles[1] not in wordindex):\n","      continue\n","\n","    word1 = int(wordindex[eles[0]])\n","    word2 = int(wordindex[eles[1]])\n","    humansim.append(float(eles[2]))\n","\n","    consim.append( cosine_similarity( embed[word1] , embed[word2] ) )\n","\n","  cor3, pvalue3 = spearmanr(humansim, consim)\n","\n","  return cor3"]},{"cell_type":"code","source":["def get_scores(word_embeddings):\n","\n","  f = open(to_path(\"voca\", version, \"voca.txt\"))\n","  line = f.readline()\n","\n","  wordindex = dict()\n","  index = 0\n","\n","  while line:\n","      word = line.strip().split()[0]\n","      wordindex[word] = index\n","      index = index + 1\n","      line = f.readline()\n","\n","  f.close()\n","\n","  sp1 = scorefunction1(word_embeddings, wordindex)\n","  sp2 = scorefunction2(word_embeddings, wordindex)\n","  sp3 = scorefunction3(word_embeddings, wordindex)\n","\n","  return ( sp1, sp2, sp3, mean([sp1, sp2, sp3]) )"],"metadata":{"id":"blhKZm0JkT8c","executionInfo":{"status":"ok","timestamp":1688990280264,"user_tz":-120,"elapsed":6,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# Model & Loss"],"metadata":{"id":"6sN8m5qNpE2k"}},{"cell_type":"code","execution_count":31,"metadata":{"id":"hVItdGzZ0wO3","executionInfo":{"status":"ok","timestamp":1688990280264,"user_tz":-120,"elapsed":5,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["class Model(nn.Module):\n","\n","  def __init__(self, n_embed, word2id):\n","    super().__init__()\n","\n","    self.word2id = word2id\n","    self.id2word = {id : word for word, id in word2id.items()}\n","\n","    self.n_vocab = len(word2id)\n","    self.n_embed = n_embed\n","\n","    self.in_embed = nn.Embedding(self.n_vocab, n_embed, dtype=torch.float64)\n","    self.out_embed = nn.Embedding(self.n_vocab, n_embed, dtype=torch.float64)\n","\n","    self.in_embed.weight.data.uniform_(-1, 1)\n","    self.out_embed.weight.data.uniform_(-1, 1)\n","\n","  def forward_input(self, input_words): # takes a batch of input words and returns their embeddings.\n","    input_vector = self.in_embed(input_words)\n","    return input_vector\n","\n","  def forward_output(self, output_words): # takes a batch of output words and returns their embeddings.\n","    output_vector = self.out_embed(output_words)\n","    return output_vector\n","\n","  def input_embeddings(self): # returns the input embeddings as a numpy array\n","    return self.in_embed.weight.data.cpu().numpy()\n","\n","  def output_embeddings(self): # returns the output embeddings as a numpy array\n","    return self.out_embed.weight.data.cpu().numpy()\n","\n","  def embedding_input_dictionary(self): # return dictionary that map words to their corresponding input embeddings\n","    embedding = self.in_embed.weight.cpu().data.numpy()\n","    E = { w : embedding[wid] for wid, w in self.id2word.items()}\n","    return E\n","\n","  def embedding_output_dictionary(self): # return dictionary that map words to their corresponding input embeddings\n","    embedding = self.out_embed.weight.cpu().data.numpy()\n","    E = { w : embedding[wid] for wid, w in self.id2word.items()}\n","    return E\n","\n","  def get_target_embedding(self, word): # return target embedding of given word\n","    try:\n","      embedding = self.embedding_input_dictionary()\n","      return embedding[word]\n","    except KeyError:\n","      print(\"Word not defined.\")\n","      return\n","\n","  def get_context_embedding(self, word): # return target embedding of given word\n","    try:\n","      embedding = self.embedding_output_dictionary()\n","      return embedding[word]\n","    except KeyError:\n","      print(\"Word not defined.\")\n","      return\n","\n","  def cosine_similarity(self, v1, v2):\n","    sumxx, sumxy, sumyy = 0, 0, 0\n","    for i in range(len(v1)):\n","      x = v1[i]\n","      y = v2[i]\n","      sumxx += x * x\n","      sumyy += y * y\n","      sumxy += x * y\n","    return sumxy / math.sqrt(sumxx * sumyy)\n","\n","  def neighbors_from_word(self, word, return_scores = False, topk = 10):\n","    word_to_embeddings = self.embedding_input_dictionary()\n","    neighbor_words = [\"<NULL>\"] * topk\n","    neighbor_similarities = [-1.0] * topk\n","    min_neighbor = neighbor_similarities.index(min(neighbor_similarities))\n","    if word not in list(word_to_embeddings.keys()):\n","      return neighbor_words, neighbor_similarities\n","    input_emb = word_to_embeddings[word]\n","    for w in word_to_embeddings.keys():\n","      word_emb = word_to_embeddings[w]\n","      sim = self.cosine_similarity(input_emb, word_emb)\n","      if sim >= neighbor_similarities[min_neighbor]:\n","        neighbor_similarities[min_neighbor] = sim\n","        neighbor_words[min_neighbor] = w\n","        min_neighbor = neighbor_similarities.index(min(neighbor_similarities))\n","    if return_scores:\n","      return neighbor_words, neighbor_similarities\n","    else:\n","      return neighbor_words\n","\n","  def save_model(self, name):\n","    torch.save(self, name)\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"mOimcp-60wMl","executionInfo":{"status":"ok","timestamp":1688990280264,"user_tz":-120,"elapsed":5,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["class NegativeSamplingLoss(nn.Module):\n","\n","  def __init__(self):\n","      super().__init__()\n","\n","  def forward(self, input_vectors, output_vectors1, output_vectors2, noise_vectors):\n","\n","    # Retrieve the batch size and embedding size\n","    batch_size, embed_size = input_vectors.shape\n","\n","    #################################################################\n","\n","    # Reshape input and output vectors for batch matrix multiplication\n","    input_vectors   = input_vectors.view(batch_size, embed_size, 1)\n","    output_vectors1 = output_vectors1.view(batch_size, 1, embed_size)\n","    output_vectors2 = output_vectors2.view(batch_size, 1, embed_size)\n","\n","    #################################################################\n","\n","    # Compute norm of each input vector\n","    input_norms = torch.linalg.matrix_norm(input_vectors, dim=(1, 2))\n","\n","    # Compute dot product of input vectors and output vectors\n","    out_loss1 = torch.bmm(output_vectors1, input_vectors).squeeze()\n","    out_loss2 = torch.bmm(output_vectors2, input_vectors).squeeze()\n","\n","    #################################################################\n","    # POSITIVE LOSS\n","\n","    positives = torch.sub(out_loss1, out_loss2, alpha=1)\n","    positives = torch.div(positives, input_norms)\n","    positives = torch.nn.functional.softplus(positives) # v16v - no neg()\n","    positives = torch.sigmoid(positives)\n","    #positives = torch.mul(positives, 2)\n","    positives = positives.squeeze()\n","\n","    #################################################################\n","\n","    loss = -(positives).mean()\n","    return loss"]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"mQIs4vQ5pHWe"}},{"cell_type":"code","source":["def print_progress_bar(curr, total, start_time, bar_length=80):\n","\n","  progress = float(curr) / float(total)\n","  arrow = '=' * int(round(progress * bar_length)-1)\n","  spaces = '-' * (bar_length - len(arrow))\n","\n","  elapsed_time = time.time() - start_time\n","  steps_per_second = float(curr) / float(elapsed_time)\n","  remaining_steps = total - curr\n","  eta_seconds = float(remaining_steps) / steps_per_second\n","  eta = time.strftime(\"%H:%M:%S\", time.gmtime(eta_seconds))\n","\n","  print(\"\\r%.2f%% [%s%s] - ETA: %s\" % ((progress * 100), arrow, spaces, eta), end='')"],"metadata":{"id":"uL71BNbV5SNp","executionInfo":{"status":"ok","timestamp":1688990280264,"user_tz":-120,"elapsed":5,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def print_status(e, epochs, curr_batch, n_batches, epoch_loss, curr_scores):\n","  epoch_loss = (epoch_loss / curr_batch)\n","  epoch_progress = (( curr_batch * 100.0 ) / (n_batches))\n","  print(\"\\rEpoch: %d/%d [Epoch Progress: %.1f%%]\" % (e + 1, epochs, epoch_progress))\n","  print(\"[LOSS: %.4f] [SF1: %.4f] [SF2: %.4f] [SF3: %.4f] [AVG_SF: %.4f]\" % (epoch_loss, curr_scores[0], curr_scores[1], curr_scores[2], curr_scores[3]))\n","  print(\"------------------------------------------------------------\")"],"metadata":{"id":"d_N2VbUpoqMc","executionInfo":{"status":"ok","timestamp":1688990280265,"user_tz":-120,"elapsed":6,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["model = Model(embedding_dim, vocab_to_int).to(device)\n","\n","criterion = NegativeSamplingLoss()\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = lr_decay_step, gamma = lr_decay_rate)"],"metadata":{"id":"FXtooMdAkhvp","executionInfo":{"status":"ok","timestamp":1688990287339,"user_tz":-120,"elapsed":7080,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["min_loss = 100.0\n","min_loss_epoch = 0\n","\n","min_val = 100.0\n","min_val_epoch = 0\n","\n","max_score = -2.0\n","max_score_step = 0\n","\n","array_loss=[]\n","array_val=[]\n","array_step=[]\n","array_scores=[]\n","\n","testwords = [\"car\", \"automobile\", \"pomegranate\"]\n","testwords_similarities = []"],"metadata":{"id":"fiFobSQlfATn","executionInfo":{"status":"ok","timestamp":1688990287340,"user_tz":-120,"elapsed":8,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","execution_count":37,"metadata":{"id":"hAYeJxaP0wJ2","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1688991270409,"user_tz":-120,"elapsed":983075,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"ccb1b3aa-01ee-4874-cc74-0ee50bd27dcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\rEpoch: 1/3 [Epoch Progress: 0.0%]\n","[LOSS: -0.6764] [SF1: 0.0408] [SF2: 0.0419] [SF3: 0.0717] [AVG_SF: 0.0515]\n","------------------------------------------------------------\n","Epoch: 1/3 [Epoch Progress: 20.0%]\n","[LOSS: -0.6791] [SF1: 0.0587] [SF2: 0.0408] [SF3: 0.1112] [AVG_SF: 0.0702]\n","------------------------------------------------------------\n","Epoch: 1/3 [Epoch Progress: 40.0%]\n","[LOSS: -0.6815] [SF1: 0.0348] [SF2: 0.0231] [SF3: 0.0902] [AVG_SF: 0.0494]\n","------------------------------------------------------------\n","Epoch: 1/3 [Epoch Progress: 60.0%]\n","[LOSS: -0.6838] [SF1: 0.0484] [SF2: 0.0439] [SF3: 0.1008] [AVG_SF: 0.0644]\n","------------------------------------------------------------\n","Epoch: 1/3 [Epoch Progress: 80.0%]\n","[LOSS: -0.6861] [SF1: 0.0775] [SF2: 0.0599] [SF3: 0.1066] [AVG_SF: 0.0813]\n","------------------------------------------------------------\n","Epoch: 1/3 [Epoch Progress: 100.0%]\n","[LOSS: -0.6882] [SF1: 0.0591] [SF2: 0.0548] [SF3: 0.0727] [AVG_SF: 0.0622]\n","------------------------------------------------------------\n","End of epoch: 1/3\n","[LOSS: -0.6882] [VAL LOSS: -0.6984]\n","------------------------------------------------------------\n","Epoch: 2/3 [Epoch Progress: 0.0%]\n","[LOSS: -0.6986] [SF1: 0.0593] [SF2: 0.0551] [SF3: 0.0729] [AVG_SF: 0.0624]\n","------------------------------------------------------------\n","Epoch: 2/3 [Epoch Progress: 20.0%]\n","[LOSS: -0.7108] [SF1: 0.0453] [SF2: 0.0368] [SF3: 0.0610] [AVG_SF: 0.0477]\n","------------------------------------------------------------\n","Epoch: 2/3 [Epoch Progress: 40.0%]\n","[LOSS: -0.7124] [SF1: 0.0555] [SF2: 0.0493] [SF3: 0.0625] [AVG_SF: 0.0558]\n","------------------------------------------------------------\n","Epoch: 2/3 [Epoch Progress: 60.0%]\n","[LOSS: -0.7139] [SF1: 0.0736] [SF2: 0.0673] [SF3: 0.0691] [AVG_SF: 0.0700]\n","------------------------------------------------------------\n","Epoch: 2/3 [Epoch Progress: 80.0%]\n","[LOSS: -0.7152] [SF1: 0.0807] [SF2: 0.0783] [SF3: 0.0412] [AVG_SF: 0.0667]\n","------------------------------------------------------------\n","Epoch: 2/3 [Epoch Progress: 100.0%]\n","[LOSS: -0.7165] [SF1: 0.0692] [SF2: 0.0780] [SF3: 0.0240] [AVG_SF: 0.0571]\n","------------------------------------------------------------\n","End of epoch: 2/3\n","[LOSS: -0.7165] [VAL LOSS: -0.7158]\n","------------------------------------------------------------\n","Epoch: 3/3 [Epoch Progress: 0.0%]\n","[LOSS: -0.7217] [SF1: 0.0692] [SF2: 0.0779] [SF3: 0.0238] [AVG_SF: 0.0569]\n","------------------------------------------------------------\n","Epoch: 3/3 [Epoch Progress: 20.0%]\n","[LOSS: -0.7274] [SF1: 0.0467] [SF2: 0.0607] [SF3: 0.0033] [AVG_SF: 0.0369]\n","------------------------------------------------------------\n","75.84% [============================================================--------------------] - ETA: 00:05:13"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-3d2f4b0d9bd5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m############ For each batch ############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_words\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_samp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_batch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-a49685898620>\u001b[0m in \u001b[0;36mget_batches\u001b[0;34m(words, batch_size, win_size, neg_samp, idx_to_contexts)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_words1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_samp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_words1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnegative_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m           \u001b[0mnegative_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# negative sample, make it different than context words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["n_batches_train, n_batches_val = (len(train_words) // batch_size) , (len(validation_words) // validation_batch_size)\n","n_batches_total = n_batches_train + n_batches_val\n","print_every = int(n_batches_train / 5)\n","\n","steps = 0\n","start_time = time.time()\n","\n","############ For each epoch ############\n","for e in range(epochs):\n","\n","  curr_batch = 0\n","  epoch_loss = 0.0\n","\n","  ############ For each batch ############\n","  for targets, contexts1, contexts2, noise_words in get_batches(train_words, batch_size, win_size, neg_samp, idx_to_contexts):\n","\n","    steps, curr_batch = steps+1, curr_batch+1\n","\n","    input_vectors = model.forward_input(to_tensor(targets))\n","    output_vectors1 = model.forward_output(to_tensor(contexts1))\n","    output_vectors2 = model.forward_output(to_tensor(contexts2))\n","    noise_vectors = model.forward_output(to_tensor(noise_words))\n","\n","    loss = criterion(input_vectors, output_vectors1, output_vectors2, noise_vectors)\n","    epoch_loss += loss.item()\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    ############ Print & Check Scores ############\n","    if curr_batch % print_every == 1:\n","\n","      word_embeddings = model.input_embeddings()\n","      sp1, sp2, sp3, average_score = get_scores(word_embeddings)\n","\n","      array_step.append(steps)\n","      array_scores.append((sp1, sp2, sp3, average_score))\n","\n","      print_status(e, epochs, curr_batch, n_batches_train, epoch_loss, array_scores[-1])\n","\n","      if average_score >= max_score:\n","        max_score_step = steps\n","        max_score = average_score\n","        model.save_model(to_path(\"models\", version, \"max_score_model.torch\"))\n","\n","      t_v = [model.get_target_embedding(tw) for tw in testwords]\n","      testwords_similarities.append( ( cosine_similarity(t_v[0], t_v[1]), cosine_similarity(t_v[0], t_v[2]), cosine_similarity(t_v[1], t_v[2]) ) )\n","\n","    print_progress_bar(steps, n_batches_total * epochs, start_time)\n","\n","  ############ Save model every epoch ############\n","  model.save_model(to_path(\"models\", version, \"epoch_%d.torch\" % (e+1)))\n","\n","  ############ Epoch loss ############\n","  epoch_loss = epoch_loss / curr_batch\n","  array_loss.append(epoch_loss)\n","  if epoch_loss <= min_loss:\n","    min_loss_epoch = e+1\n","    min_loss = epoch_loss\n","    model.save_model(to_path(\"models\", version, \"min_loss_model.torch\"))\n","\n","  ############ Validation loss ############\n","  epoch_val_loss, curr_batch = 0.0, 0\n","  for trg, c1, c2, nw in get_batches(validation_words, validation_batch_size, win_size, neg_samp, idx_to_contexts):\n","    steps, curr_batch = steps+1, curr_batch+1\n","    trg, c1, c2, nw = to_tensor(trg), to_tensor(c1), to_tensor(c2), to_tensor(nw)\n","    in_v, o_v1, o_v2, n_s = model.forward_input(trg), model.forward_output(c1), model.forward_output(c2), model.forward_output(nw)\n","    epoch_val_loss += criterion(in_v, o_v1, o_v2, n_s).item()\n","    print_progress_bar(steps, n_batches_total * epochs, start_time)\n","\n","  epoch_val_loss = epoch_val_loss / curr_batch\n","  array_val.append(epoch_val_loss)\n","  print(\"\\rEnd of epoch: %d/%d\" % (e + 1, epochs))\n","  print(\"[LOSS: %.4f] [VAL LOSS: %.4f]\" % (epoch_loss, epoch_val_loss))\n","  print(\"------------------------------------------------------------\")\n","\n","  if epoch_val_loss <= min_val:\n","    min_val_epoch = e+1\n","    min_val = epoch_val_loss\n","    model.save_model(to_path(\"models\", version, \"min_val_model.torch\"))\n","\n","  ############ LR Scheduling ############\n","  scheduler.step()\n","\n","\n","\n","print(\"\\n[COMPLETED] : %s\" % (time.strftime(\"%H hours, %M minutes, %S seconds\", time.gmtime(time.time() - start_time))))\n"]},{"cell_type":"markdown","source":["# Save model"],"metadata":{"id":"N5OWHvY_Orjj"}},{"cell_type":"code","source":["model.save_model(to_path(\"models\", version, \"final_model.torch\"))"],"metadata":{"id":"kLPQ65D2yqly","executionInfo":{"status":"aborted","timestamp":1688991270410,"user_tz":-120,"elapsed":15,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nnx9W2C5Tkg1"},"source":["# Results"]},{"cell_type":"code","source":["print(\"Training peaked for score functions at step: %d, with average score %.3f\" % ( max_score_step , max_score ))\n","print(\"Training peaked for min loss at epoch %d, with loss %.3f\" % ( min_loss_epoch , min_loss ))\n","print(\"Training peaked for min validation loss at epoch %d, with val loss %.3f\" % ( min_val_epoch , min_val ))"],"metadata":{"id":"3Azb9EiIUV1S","executionInfo":{"status":"aborted","timestamp":1688991270411,"user_tz":-120,"elapsed":16,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 5), sharex = True, sharey = True)\n","\n","ax1.grid(axis='both')\n","ax1.plot(array_step, [score[0] for score in array_scores], label = \"sf1\")\n","ax1.plot(array_step, [score[1] for score in array_scores], label = \"sf2\")\n","ax1.plot(array_step, [score[2] for score in array_scores], label = \"sf3\")\n","ax1.legend()\n","\n","ax2.grid(axis='both')\n","ax2.plot(array_step, [score[3] for score in array_scores], color = \"red\", label = \"average\", lw = 3.0)\n","ax2.legend()\n","\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Correlation (Spearman)\")\n","fig.tight_layout()\n","fig.show()"],"metadata":{"id":"uf5aMELmk-9e","executionInfo":{"status":"aborted","timestamp":1688991270412,"user_tz":-120,"elapsed":16,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Hohvtn1SZkJ","executionInfo":{"status":"aborted","timestamp":1688991270413,"user_tz":-120,"elapsed":17,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["plt.plot(range(1, epochs+1), array_loss, label = \"training\")\n","plt.plot(range(1, epochs+1), array_val, label = \"validation\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.title(\"Training/Validation Loss\")\n","plt.grid(axis='both')\n","plt.show()"]},{"cell_type":"code","source":["plt.plot(array_step, [item[0] for item in testwords_similarities], label = (\"%s vs %s\" % (testwords[0], testwords[1])))\n","plt.plot(array_step, [item[1] for item in testwords_similarities], label = (\"%s vs %s\" % (testwords[0], testwords[2])))\n","plt.plot(array_step, [item[2] for item in testwords_similarities], label = (\"%s vs %s\" % (testwords[1], testwords[2])))\n","\n","plt.xlabel(\"Step\")\n","plt.ylabel(\"Cosine similarity [-1,+1]\")\n","plt.legend()\n","plt.grid(axis='both')\n","plt.show()"],"metadata":{"id":"oqDskyUUmJ-R","executionInfo":{"status":"aborted","timestamp":1688991270413,"user_tz":-120,"elapsed":17,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# What if we combine target and context embeddings?"],"metadata":{"id":"52uwCiKBR69Q"}},{"cell_type":"code","source":["trg_embeddings = model.input_embeddings()\n","s1, s2, s3, av = get_scores(trg_embeddings)\n","\n","print(\"Target embeddings: [SP1: %.3f] [SP2: %.3f] [SP3: %.3f] [AVG: %.3f]\" % (s1, s2, s3, av))"],"metadata":{"id":"RsRhBQ15THOB","executionInfo":{"status":"aborted","timestamp":1688991270414,"user_tz":-120,"elapsed":18,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ctx_embeddings = model.output_embeddings()\n","s1, s2, s3, av = get_scores(ctx_embeddings)\n","\n","print(\"Context embeddings: [SP1: %.3f] [SP2: %.3f] [SP3: %.3f] [AVG: %.3f]\" % (s1, s2, s3, av))"],"metadata":{"id":"VR2AWci5TYCR","executionInfo":{"status":"aborted","timestamp":1688991270417,"user_tz":-120,"elapsed":20,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_embeddings = (trg_embeddings + ctx_embeddings) / 2.0\n","s1, s2, s3, av = get_scores(combined_embeddings)\n","\n","print(\"Combined embeddings: [SP1: %.3f] [SP2: %.3f] [SP3: %.3f] [AVG: %.3f]\" % (s1, s2, s3, av))"],"metadata":{"id":"a7Z42cq2R7Vp","executionInfo":{"status":"aborted","timestamp":1688991270418,"user_tz":-120,"elapsed":21,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Closest neighbors"],"metadata":{"id":"RprNMZgbEFad"}},{"cell_type":"code","source":["test_words = [\"car\", \"money\", \"flower\", \"japan\", \"suspicious\"]\n","\n","table = PrettyTable()\n","for w in test_words:\n","  table.add_column(\"'%s'\" % (w) , model.neighbors_from_word(w))\n","\n","print(table)"],"metadata":{"id":"ub0NoYJB0iY1","executionInfo":{"status":"aborted","timestamp":1688991270419,"user_tz":-120,"elapsed":22,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Disconnect from runtime"],"metadata":{"id":"iXGFfskVYpJ8"}},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"NYjUszocYrq-","executionInfo":{"status":"aborted","timestamp":1688991270419,"user_tz":-120,"elapsed":22,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm","authorship_tag":"ABX9TyNYHVIIbbh4G0TrTbNigeef"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}