{"cells":[{"cell_type":"markdown","metadata":{"id":"o2hCUF1-tXAo"},"source":["# Training word embeddings with the SGNS algorithm\n","\n","In this notebook, we'll see a PyTorch implementation of a well-known training algorithm for word embeddings, Mikolov's [Skip-gram with negative sampling](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n","\n","Please note that the example is somewhat incomplete, because in a realistic implementation we would also *save* the embeddings when training is finished. In this implementation, we'll just print the similarities to some test instances."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERZ4qL2wtXAr"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","\n","import sys, time, os\n","from collections import Counter"]},{"cell_type":"markdown","metadata":{"id":"Cc6t5ozwtXA6"},"source":["If you'd like to run the code, download [this package](http://www.cse.chalmers.se/~richajo/dit865/slask/files/wikipedia_small.zip) and unzip it. [Colab users: it can be a bit difficult to download large files to the local directory in Colab, so if you have problems executing the following cell because the download gets stuck, you might need to mount a Drive directory instead.]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166129,"status":"ok","timestamp":1684342774859,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"},"user_tz":-120},"id":"1-hzJ5PDtg-E","outputId":"e2d562d3-8045-44b1-b9c6-c16236620018"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-05-17 16:56:47--  http://www.cse.chalmers.se/~richajo/dit865/slask/files/wikipedia_small.zip\n","Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.221.33\n","Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.221.33|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://www.cse.chalmers.se/~richajo/dit865/slask/files/wikipedia_small.zip [following]\n","--2023-05-17 16:56:48--  https://www.cse.chalmers.se/~richajo/dit865/slask/files/wikipedia_small.zip\n","Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.221.33|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 48419133 (46M) [application/zip]\n","Saving to: ‘wikipedia_small.zip’\n","\n","wikipedia_small.zip 100%[===================>]  46.18M  1.47MB/s    in 2m 43s  \n","\n","2023-05-17 16:59:32 (291 KB/s) - ‘wikipedia_small.zip’ saved [48419133/48419133]\n","\n","Archive:  wikipedia_small.zip\n","   creating: wikipedia_small/\n","  inflating: wikipedia_small/wikipedia.txt  \n","sample_data\twikipedia_small      wordsim_similarity_goldstandard.txt\n","SimLex-999.txt\twikipedia_small.zip\n"]}],"source":["!rm -rf wikipedia* *.zip*\n","!wget http://www.cse.chalmers.se/~richajo/dit865/slask/files/wikipedia_small.zip\n","!unzip wikipedia_small.zip\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"fgEVbcOAtXBA"},"source":["## Preliminaries: building the vocabulary and negative sampling table\n","\n","We will first make a function that goes through the training corpus and finds the most frequent words, which will be used for the vocabulary. A special dummy token will be used as a stand-in for the words that are less frequent.\n","\n","In addition, we will create the table that will be used for *negative sampling*. Each word will be sampled with a probability that is proportional to its frequency to the power of a constant (called `ns_exp` here). Sampling words randomly can be a bit tricky to implement efficiently, and we'll use a trick that was used in the `word2vec` software: we'll make a large array where each word will occur a number of times that is roughly proportional to its probability."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aElUfUJttXBD"},"outputs":[],"source":["\n","def make_ns_table(params):\n","    corpus = params['corpus']\n","    voc_size = params['voc-size']\n","    ns_table_size = params['ns-table-size']\n","    unk_str = params['unknown-str']\n","    lowercase = params['lowercase']\n","    ns_exp = params['ns-exp']\n","\n","    # This is what we'll use to store the frequencies.\n","    freqs = Counter()\n","\n","    print('Building vocabulary and sampling table...')\n","\n","    # First, build a full frequency table from the whole corpus.\n","    with open(corpus) as f:\n","        for i, line in enumerate(f, 1):\n","            if lowercase:\n","                line = line.lower()\n","            freqs.update(line.split())\n","            if i % 50000 == 0:\n","                sys.stdout.write('.')\n","                sys.stdout.flush()\n","            if i % 1000000 == 0:\n","                sys.stdout.write(' ')\n","                sys.stdout.write(str(i))\n","                sys.stdout.write('\\n')\n","                sys.stdout.flush()\n","    print()\n","\n","    # Sort the frequencies, then select the most frequent words as the vocabulary.\n","    freqs_sorted = sorted(freqs.items(),\n","                          key=lambda p: (p[1], p[0]),\n","                          reverse=True)\n","    if len(freqs_sorted) > voc_size-1:\n","        sum_freq_pruned = sum(f for _, f in freqs_sorted[voc_size-1:])\n","    else:\n","        sum_freq_pruned = 1\n","\n","    # We'll add a special dummy to represent the occurrences of low-frequency words.\n","    freqs_sorted = [(unk_str, sum_freq_pruned)] + freqs_sorted[:voc_size-1]\n","\n","    # Now, we'll compute the negative sampling table.\n","    # The negative sampling probabilities are proportional to the frequencies\n","    # to the power of a constant (typically 0.75).\n","    ns_table = {}\n","    sum_freq = 0\n","    for w, freq in freqs_sorted:\n","        ns_freq = freq ** ns_exp\n","        ns_table[w] = ns_freq\n","        sum_freq += ns_freq\n","\n","    # Convert the negative sampling probabilities to integers, in order to make\n","    # sampling a bit faster and easier.\n","    # We return a list of tuples consisting of:\n","    # - the word\n","    # - its frequency in the training data\n","    # - the number of positions reserved for this word in the negative sampling table\n","    scaler = ns_table_size / sum_freq\n","    return [(w, freq, int(round(ns_table[w]*scaler))) for w, freq in freqs_sorted]\n"]},{"cell_type":"markdown","metadata":{"id":"4ecby0GQtXBO"},"source":["And then two utility functions to load and save the negative sampling table."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJAXqLBvtXBQ"},"outputs":[],"source":["def load_ns_table(filename):\n","    with open(filename) as f:\n","        out = []\n","        for l in f:\n","            t = l.split()\n","            out.append((t[0], int(t[1]), int(t[2])))\n","        return out\n","\n","def save_ns_table(table, filename):\n","    with open(filename, 'w') as f:\n","        for w, fr, ns in table:\n","            print(f'{w} {fr} {ns}', file=f)"]},{"cell_type":"markdown","metadata":{"id":"cenMMINctXBb"},"source":["## Generating target–context pairs\n","\n","The following class is used to go through the training file line by line, and generate positive training instances (pairs consisting of a target word and a context word). Here, we will use all the preprocessing intricacies described in [Mikolov's paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n","\n","The `batches` method will generate one batch at a time, containing a number of positive training instances coded as integers. The negative training instances will be created elsewhere."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvQj7MkgtXBl"},"outputs":[],"source":["class SGNSContextGenerator:\n","\n","    def __init__(self, ns_table, params):\n","\n","        # The name of the training file.\n","        self.corpus = params['corpus']\n","\n","        # The string-to-integer mapping for the vocabulary.\n","        self.voc = { w:i for i, (w, _, _ ) in enumerate(ns_table) }\n","\n","        # The number of positive instances we'll create in each batch.\n","        self.batch_size = params['batch-size']\n","\n","        # The maximal width of the context window.\n","        self.ctx_width = params['context-width']\n","\n","        # Whether we should\n","        self.lowercase = params['lowercase']\n","\n","        self.word_count = 0\n","\n","        # We define the pruning probabilities for each word as in Mikolov's paper.\n","        total_freq = sum(f for _, f, _ in ns_table)\n","        self.prune_probs = {}\n","        for w, f, _ in ns_table:\n","            self.prune_probs[w] = 1 - np.sqrt(params['prune-threshold'] * total_freq / f)\n","\n","    def prune(self, tokens):\n","        ps = np.random.random(size=len(tokens))\n","        # Remove some words from the input with probabilities defined by their frequencies.\n","        return [ w for w, p in zip(tokens, ps) if p >= self.prune_probs.get(w, 0) ]\n","\n","    def batches(self):\n","\n","        widths = np.random.randint(1, self.ctx_width+1, size=self.batch_size)\n","        width_ix = 0\n","\n","        self.word_count = 0\n","\n","        with open(self.corpus) as f:\n","            out_t = []\n","            out_c = []\n","            for line in f:\n","\n","                # Process one line: lowercase and split into tokens.\n","                if self.lowercase:\n","                    line = line.lower()\n","                tokens = line.split()\n","                self.word_count += len(tokens)\n","\n","                # Remove some words, then encode as integers.\n","                encoded = [ self.voc.get(t, 0) for t in self.prune(tokens) ]\n","\n","                for i, t in enumerate(encoded):\n","\n","                    # The context width is selected uniformly between 1 and the maximal width.\n","                    w = widths[width_ix]\n","                    width_ix += 1\n","\n","                    # Compute start and end positions for the context.\n","                    start = max(0, i-w)\n","                    end = min(i+w+1, len(encoded))\n","\n","                    # Finally, generate target--context pairs.\n","                    for j in range(start, end):\n","                        if j != i:\n","                            out_t.append(encoded[i])\n","                            out_c.append(encoded[j])\n","\n","                            # If we've generate enough pairs, yield a batch.\n","                            # Each batch is a list of targets and a list of corresponding contexts.\n","                            if len(out_t) == self.batch_size:\n","                                yield out_t, out_c\n","\n","                                # After coming back, reset the batch.\n","                                widths = np.random.randint(1, self.ctx_width+1, size=self.batch_size)\n","                                width_ix = 0\n","                                out_t = []\n","                                out_c = []\n","\n","            print('End of file.')\n","            if len(out_t) > 0:\n","                # Yield the final batch.\n","                yield out_t, out_c"]},{"cell_type":"markdown","metadata":{"id":"6k92EkA5tXBt"},"source":["## Defining the model\n","\n","Next, we implement the neural network that defines the model. The parameters just consist of two sets of embeddings: one for the target words, and one for the contexts.\n","\n","The forward step is fairly trivial: we just compute the dot products of the target and context embeddings. As usual, the most annoying part is to keep track of the tensor shapes.\n","\n","We also add a couple of methods that allow us to inspect the model: computing the cosine similarity between the embeddings for two words, and finding the nearest neighbor lists of a set of words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uha5Zs7RtXBw"},"outputs":[],"source":["class SGNSModel(nn.Module):\n","\n","    def __init__(self, voc, params):\n","        super().__init__()\n","\n","        voc_size = len(voc)\n","\n","        # Target word embeddings\n","        self.w = nn.Embedding(voc_size, params['emb-dim'])\n","        # Context embeddings\n","        self.c = nn.Embedding(voc_size, params['emb-dim'])\n","\n","        # Some things we need to print nearest neighbor lists for diagnostics.\n","        self.voc = voc\n","        self.ivoc = { i:w for w, i in voc.items() }\n","\n","    def forward(self, tgt, ctx):\n","        # tgt is a 1-dimensional tensor containing target word ids\n","        # ctx is a 2-dimensional tensor containing positive and negative context ids for each target\n","\n","        # Look up the embeddings for the target words.\n","        # shape: (batch size, embedding dimension)\n","        tgt_emb = self.w(tgt)\n","\n","        n_batch, emb_dim = tgt_emb.shape\n","        n_ctx = ctx.shape[1]\n","\n","        # View this as a 3-dimensional tensor, with\n","        # shape (batch size, 1, embedding dimension)\n","        tgt_emb = tgt_emb.view(n_batch, 1, emb_dim)\n","\n","        # Look up the embeddings for the positive and negative context words.\n","        # shape: (batch size, nbr contexts, emb dim)\n","        ctx_emb = self.c(ctx)\n","\n","        # Transpose the tensor for matrix multiplication\n","        # shape: (batch size, emb dim, nbr contexts)\n","        ctx_emb = ctx_emb.transpose(1, 2)\n","\n","        # Compute the dot products between target word embeddings and context\n","        # embeddings. We express this as a batch matrix multiplication (bmm).\n","        # shape: (batch size, 1, nbr contexts)\n","        dots = tgt_emb.bmm(ctx_emb)\n","\n","        # View this result as a 2-dimensional tensor.\n","        # shape: (batch size, nbr contexts)\n","        dots = dots.view(n_batch, n_ctx)\n","\n","        return dots\n","\n","    ############ [ALTERED] [0] ############\n","    def context_nearest_neighbors(self, words, n_neighbors):\n","\n","        # Encode the words as integers, and put them into a PyTorch tensor.\n","        words_ix = torch.as_tensor([self.voc[w] for w in words])\n","\n","        # Look up the embeddings for the test words.\n","        voc_size, emb_dim = self.c.weight.shape\n","        test_emb = self.c(words_ix).view(len(words), 1, emb_dim)\n","\n","        # Also, get the embeddings for all words in the vocabulary.\n","        all_emb = self.c.weight.view(1, voc_size, emb_dim)\n","\n","        # We'll use a cosine similarity function to find the most similar words.\n","        # The .view kludgery above is needed for the batch-wise cosine similarity.\n","        sim_func = nn.CosineSimilarity(dim=2)\n","        scores = sim_func(test_emb, all_emb)\n","        # The shape of scores is (nbr of test words, total number of words)\n","\n","        # Find the top-scoring columns in each row.\n","        if not n_neighbors:\n","            n_neighbors = self.n_testwords_neighbors\n","        near_nbr = scores.topk(n_neighbors+1, dim=1)\n","        values = near_nbr.values[:,1:]\n","        indices = near_nbr.indices[:, 1:]\n","\n","        # Finally, map word indices back to strings, and put the result in a list.\n","        out = []\n","        for ixs, vals in zip(indices, values):\n","            out.append([ (self.ivoc[ix.item()], val.item()) for ix, val in zip(ixs, vals) ])\n","        return out\n","\n","    def context_cosine_similarity(self, word1, word2):\n","        # We just look up the two embeddings and use PyTorch's built-in cosine similarity.\n","        v1 = self.c(torch.as_tensor(self.voc[word1]))\n","        v2 = self.c(torch.as_tensor(self.voc[word2]))\n","        sim = nn.CosineSimilarity(dim=0)\n","        return sim(v1, v2).item()\n","\n","    def nearest_neighbors(self, words, n_neighbors):\n","\n","        # Encode the words as integers, and put them into a PyTorch tensor.\n","        words_ix = torch.as_tensor([self.voc[w] for w in words])\n","\n","        # Look up the embeddings for the test words.\n","        voc_size, emb_dim = self.w.weight.shape\n","        test_emb = self.w(words_ix).view(len(words), 1, emb_dim)\n","\n","        # Also, get the embeddings for all words in the vocabulary.\n","        all_emb = self.w.weight.view(1, voc_size, emb_dim)\n","\n","        # We'll use a cosine similarity function to find the most similar words.\n","        # The .view kludgery above is needed for the batch-wise cosine similarity.\n","        sim_func = nn.CosineSimilarity(dim=2)\n","        scores = sim_func(test_emb, all_emb)\n","        # The shape of scores is (nbr of test words, total number of words)\n","\n","        # Find the top-scoring columns in each row.\n","        if not n_neighbors:\n","            n_neighbors = self.n_testwords_neighbors\n","        near_nbr = scores.topk(n_neighbors+1, dim=1)\n","        values = near_nbr.values[:,1:]\n","        indices = near_nbr.indices[:, 1:]\n","\n","        # Finally, map word indices back to strings, and put the result in a list.\n","        out = []\n","        for ixs, vals in zip(indices, values):\n","            out.append([ (self.ivoc[ix.item()], val.item()) for ix, val in zip(ixs, vals) ])\n","        return out\n","\n","\n","    def cosine_similarity(self, word1, word2):\n","        # We just look up the two embeddings and use PyTorch's built-in cosine similarity.\n","        v1 = self.w(torch.as_tensor(self.voc[word1]))\n","        v2 = self.w(torch.as_tensor(self.voc[word2]))\n","        sim = nn.CosineSimilarity(dim=0)\n","        return sim(v1, v2).item()\n"]},{"cell_type":"markdown","metadata":{"id":"1rLDBfrgtXB5"},"source":["## Training\n","\n","The following class contains the training loop: it creates a batch of positive target–context pairs, generates negative samples, and then updates the embedding model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWSweIXhtXB7"},"outputs":[],"source":["class SGNSTrainer:\n","\n","    def __init__(self, instance_gen, model, ns_table, params):\n","        self.instance_gen = instance_gen\n","        self.model = model\n","        self.n_epochs = params['n-epochs']\n","        self.max_words = params.get('max-words')\n","        n_batch = params['batch-size']\n","        self.n_ns = params['n-neg-samples']\n","\n","        if params['optimizer'] == 'adam':\n","            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=params['lr'])\n","        elif params['optimizer'] == 'sgd':\n","            self.optimizer = torch.optim.SGD(self.model.parameters(), lr=params['lr'])\n","\n","        # We'll use a binary cross-entropy loss, since we have a binary classification problem:\n","        # distinguishing positive from negative contexts.\n","        self.loss = nn.BCEWithLogitsLoss()\n","\n","        # Build the negative sampling table.\n","        ns_table_expanded = []\n","        for i, (_, _, count) in enumerate(ns_table):\n","            ns_table_expanded.extend([i] * count)\n","        self.ns_table = torch.as_tensor(ns_table_expanded)\n","\n","        # Define the \"gold standard\" that we'll use to compute the loss.\n","        # It consists of a column of ones, and then a number of columns of zeros.\n","        # This structure corresponds to the positive and negative contexts, respectively.\n","        y_pos = torch.ones((n_batch, 1))\n","        y_neg = torch.zeros((n_batch, self.n_ns))\n","        self.y = torch.cat([y_pos, y_neg], dim=1)\n","\n","        # Some things we need to print nearest neighbor lists for diagnostics.\n","        #self.voc = instance_gen.voc\n","        #self.ivoc = { i:w for w, i in self.voc.items() }\n","        self.testwords = params['testwords']\n","        self.n_testwords_neighbors = params['n-testwords-neighbors']\n","\n","        self.epoch = 0\n","\n","    def print_test_nearest_neighbors(self):\n","\n","        nn_lists = self.model.nearest_neighbors(self.testwords, self.n_testwords_neighbors)\n","\n","        # For each test word, print the most similar words.\n","        for w, nn_list in zip(self.testwords, nn_lists):\n","            print(w, end=':\\n')\n","            for nn, sim in nn_list:\n","                print(f' {nn} ({sim:.3f})', end='')\n","            print()\n","\n","        print('------------------------------------')\n","\n","    def make_negative_sample(self, batch_size):\n","        neg_sample_ixs = torch.randint(len(self.ns_table), (batch_size, self.n_ns))\n","        return self.ns_table.take(neg_sample_ixs)\n","\n","    def train(self):\n","\n","        ############ [ALTERED] [2] ############\n","        print_interval = 10000000 # 5000000\n","        ############ [ALTERED] [2] ############\n","        loss_values = [] # to store the loss values\n","\n","        while self.epoch < self.n_epochs:\n","            print(f'Epoch {self.epoch+1}.')\n","\n","            # For diagnostics.\n","            n_pairs = 0\n","            sum_loss = 0\n","            total_pairs = 0\n","            n_batches = 0\n","            t0 = time.time()\n","\n","            for t, c_pos in self.instance_gen.batches():\n","\n","                batch_size = len(t)\n","\n","                # Put the encoded target words and contexts into PyTorch tensors.\n","                t = torch.as_tensor(t)\n","                c_pos = torch.as_tensor(c_pos)\n","                c_pos = c_pos.view(batch_size, 1)\n","\n","                # Generate a sample of fake context words.\n","                # shape: (batch size, number of negative samples)\n","                c_neg = self.make_negative_sample(batch_size)\n","\n","                # Combine positive and negative contexts.\n","                # shape: (batch size, 1 + nbr neg samples)\n","                c = torch.cat([c_pos, c_neg], dim=1)\n","\n","                self.optimizer.zero_grad()\n","\n","                # Compute the output from the model.\n","                # That is, the dot products between target embeddings\n","                # and context embeddings.\n","                scores = self.model(t, c)\n","\n","                # Compute the loss with respect to the gold standard.\n","                loss = self.loss(scores, self.y[:batch_size])\n","\n","                # Compute gradients and update the embeddings.\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                # We'll print some diagnostics periodically.\n","                sum_loss += loss.item()\n","                n_pairs += batch_size\n","                n_batches += 1\n","                if n_pairs > print_interval:\n","                    total_words = self.instance_gen.word_count\n","                    total_pairs += n_pairs\n","                    t1 = time.time()\n","                    print(f'Pairs: {total_pairs}, words: {total_words}, loss: {sum_loss / n_batches:.4f}, time: {t1-t0:.2f}')\n","                    ############ [ALTERED] [3] ############\n","                    loss_values.append(sum_loss / n_batches)\n","                    self.print_test_nearest_neighbors()\n","                    n_pairs = 0\n","                    sum_loss = 0\n","                    n_batches = 0\n","                    t0 = time.time()\n","\n","            self.epoch += 1\n","\n","        ############ [ALTERED] [4] ############\n","        plt.plot(loss_values)\n","        plt.xlabel('Time')\n","        plt.ylabel('Loss')\n","        plt.title('Training Loss')\n","        plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"SXT-EuPdtXCB"},"source":["## Putting all the pieces together\n","\n","Now, we have all the pieces that we need to train the model. The following code just calls the other functions that we developed above. It also contains the parameters that control the program's behavior.\n","\n","To keep things fast, we'll just train on a small dataset. In a realistic implementation, we'd probably use a larger dataset and also run for several epochs.\n","\n","When we run this code, you will see that the similarity lists for the test words gradually start to make some sense. After an epoch, most of the lists should start to be sensible. The quality, particularly for infrequent words, will improve further if you use more training data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"49a6H3rftXCB","outputId":"da4becd9-e6d8-4869-827e-03c30cd89eb7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on CPU.\n","Building vocabulary and sampling table...\n",".................... 1000000\n","..\n","Epoch 1.\n","Pairs: 10485760, words: 2893757, loss: 1.9772, time: 95.20\n","apple:\n"," rathaus (0.563) conqueror (0.527) motive (0.522) babies (0.520) 475 (0.492)\n","terrible:\n"," creative (0.520) further (0.519) kristen (0.499) site (0.498) malick (0.491)\n","sweden:\n"," listed (0.585) primeval (0.526) trip (0.524) prominent (0.513) nation (0.507)\n","1979:\n"," ethnologue (0.602) products (0.570) formula (0.564) reducing (0.563) 4th (0.559)\n","write:\n"," perhaps (0.620) entry (0.576) effect (0.557) earlier (0.556) emphasized (0.546)\n","gothenburg:\n"," i-49 (0.520) correct (0.475) choice (0.464) 68030 (0.464) monkey (0.458)\n","------------------------------------\n"]}],"source":["model = None\n","\n","def main():\n","    global model\n","    params = {\n","        'corpus': 'wikipedia_small/wikipedia.txt', # Training data file\n","        'device': 'cuda', # Device\n","\n","        'n-neg-samples': 5, # Number of negative samples per positive sample\n","        'emb-dim': 64, # Embedding dimensionality\n","\n","        ############ [ALTERED] [5] ############\n","        'n-epochs': 10, # Number of epochs\n","\n","        'batch-size': 1<<20, # Number of positive training instances in one batch\n","        'context-width': 5, # Maximal possible context width\n","        'prune-threshold': 1e-3, # Pruning threshold (see Mikolov's paper)\n","        'voc-size': 100000, # Maximal vocabulary size\n","        'ns-table-file': 'ns_table.txt', # Where to store the negative sampling table\n","        'ns-table-size': 1<<24, # Size of negative sampling table\n","        'ns-exp': 0.75, # Smoothing parameter for negative sampling distribution (see paper)\n","        'unknown-str': '<UNKNOWN>', # Dummy token for low-frequency words\n","        'lowercase': True, # Whether to lowercase the text\n","        'optimizer': 'adam', # Which gradient descent optimizer to use\n","        'lr': 1e-1, # Learning rate for the  optimizer\n","\n","        # The test words for which we print the nearest neighbors periodically\n","        'testwords': ['apple', 'terrible', 'sweden', '1979', 'write', 'gothenburg'],\n","        # Number of nearest neighbors\n","        'n-testwords-neighbors': 5,\n","    }\n","\n","    if params['device'] == 'cuda' and torch.cuda.is_available():\n","        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","        print('Running on CUDA device.')\n","    else:\n","        torch.set_default_tensor_type(torch.FloatTensor)\n","        print('Running on CPU.')\n","\n","    # If we didn't already create the vocabulary and negative\n","    # sampling table, we'll do that now.\n","    if os.path.exists(params['ns-table-file']):\n","        ns_table = load_ns_table(params['ns-table-file'])\n","    else:\n","        ns_table = make_ns_table(params)\n","        save_ns_table(ns_table, params['ns-table-file'])\n","\n","    ctx_gen = SGNSContextGenerator(ns_table, params)\n","    model = SGNSModel(ctx_gen.voc, params)\n","    trainer = SGNSTrainer(ctx_gen, model, ns_table, params)\n","\n","    trainer.train()\n","\n","main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5ftisQ3TW7A"},"outputs":[],"source":["############ [ALTERED] [6] ############\n","# Checking if context and target embeddings are the same in the end.\n","mismatch_idx = []\n","\n","for idx, t_emb in enumerate(model.w.weight):\n","  if not ( torch.equal( t_emb , model.c.weight[idx] ) ) :\n","    mismatch_idx.append(idx)\n","\n","if len(mismatch_idx):\n","  print(\"Context and Target embeddings don't match. Mismatch on index:\")\n","  print(mismatch_idx)\n","else:\n","  print(\"Context and Target embeddings match.\")"]},{"cell_type":"markdown","metadata":{"id":"6vKCdFebtXCL"},"source":["# Inspecting the result"]},{"cell_type":"markdown","metadata":{"id":"o6_dckqGKKhi"},"source":["## Closest Words\n","### Using target vs context embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjYfjOFTtXCM"},"outputs":[],"source":["model.nearest_neighbors(['plane'], 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxIBm4WC61xl"},"outputs":[],"source":["model.context_nearest_neighbors(['plane'], 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvfC3sPP7qLz"},"outputs":[],"source":["model.nearest_neighbors(['japan'], 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6hgASDB7qCa"},"outputs":[],"source":["model.context_nearest_neighbors(['japan'], 5)"]},{"cell_type":"markdown","metadata":{"id":"RvxFH9-JtXCT"},"source":["## Cosine similarity of given word pairs\n","### Target vs context embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxIvVJA-7dW9"},"outputs":[],"source":["model.cosine_similarity('dog', 'cat')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDRAW0CRtXCU"},"outputs":[],"source":["model.context_cosine_similarity('dog', 'cat')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bufox8e_tXCa"},"outputs":[],"source":["model.cosine_similarity('dog', 'gorilla')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRYUG1MW7igR"},"outputs":[],"source":["model.context_cosine_similarity('dog', 'gorilla')"]},{"cell_type":"markdown","metadata":{"id":"liYrDksl9tEm"},"source":["## Testing Word Analogy\n","### Target Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypSU13oP9t8k"},"outputs":[],"source":["model.cosine_similarity('good', 'best')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HlsI4H2f9tjc"},"outputs":[],"source":["model.cosine_similarity('smart', 'smartest')"]},{"cell_type":"markdown","metadata":{"id":"qxg3DonUKEFJ"},"source":["### Added Vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XTmQjGvw91hp"},"outputs":[],"source":["word1 = \"good\"\n","word2 = \"best\"\n","v1 = model.w(torch.as_tensor(model.voc[word1])) + model.c(torch.as_tensor(model.voc[word1]))\n","v2 = model.w(torch.as_tensor(model.voc[word2])) + model.c(torch.as_tensor(model.voc[word2]))\n","sim = nn.CosineSimilarity(dim=0)\n","sim(v1, v2).item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ghKZyJD91Wg"},"outputs":[],"source":["word1 = \"smart\"\n","word2 = \"smartest\"\n","v1 = model.w(torch.as_tensor(model.voc[word1]))/2 + model.c(torch.as_tensor(model.voc[word1]))/2\n","v2 = model.w(torch.as_tensor(model.voc[word2]))/2 + model.c(torch.as_tensor(model.voc[word2]))/2\n","sim = nn.CosineSimilarity(dim=0)\n","sim(v1, v2).item()"]},{"cell_type":"markdown","metadata":{"id":"6oBcafaYJ8Pw"},"source":["## Silhouette Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSnZzq9Iuo-q"},"outputs":[],"source":["import numpy as np\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","\n","embeddings = model.w.weight.detach().cpu().numpy()\n","\n","# Cluster similar words together using k-means\n","n_clusters = 50  # The number of clusters to use\n","kmeans = KMeans(n_clusters=n_clusters)\n","clusters = kmeans.fit_predict(embeddings)\n","\n","# Evaluate the quality of the clustering using silhouette score\n","silhouette_avg = silhouette_score(embeddings, clusters)\n","print(\"The average silhouette score is\", silhouette_avg)\n"]},{"cell_type":"markdown","metadata":{"id":"nKyuOooqKzuz"},"source":["## Words Similarity using SimLex and WordSim\n","### Using W vs W+C"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytXQkeUj6o9E"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.stats import pearsonr\n","from scipy.stats import spearmanr\n","\n","# Load the dataset as a list of word pairs and human similarity scores\n","with open('SimLex-999.txt', 'r') as f:\n","    lines = f.readlines()\n","    # First row of SimLex is for name of the columns\n","    lines = [line.strip().split() for line in lines[1:]]\n","    word_pairs = [(line[0], line[1]) for line in lines]\n","    # SimLex similarities are [0,10], need to normalize down to [-1,1]\n","    human_scores = [(float(line[3]) / 10 * 2 - 1) for line in lines]\n","\n","# Calculate cosine similarity with target embeddings\n","similarity_scores = []\n","for pair in word_pairs:\n","    try:\n","        word1, word2 = pair\n","        similarity = model.cosine_similarity(word1, word2)\n","        similarity_scores.append(similarity)\n","    except KeyError:\n","        similarity_scores.append(-1)\n","\n","# Calculate cosine similarity with context+target embeddings\n","c_similarity_scores = []\n","for pair in word_pairs:\n","    try:\n","        word1, word2 = pair\n","        v1 = model.w(torch.as_tensor(model.voc[word1]))/2 + model.c(torch.as_tensor(model.voc[word1]))/2\n","        v2 = model.w(torch.as_tensor(model.voc[word2]))/2 + model.c(torch.as_tensor(model.voc[word2]))/2\n","        sim = nn.CosineSimilarity(dim=0)\n","        similarity = sim(v1, v2).item()\n","        c_similarity_scores.append(similarity)\n","    except KeyError:\n","        c_similarity_scores.append(-1)\n","\n","\n","pearson_coef, p_pearson = pearsonr(similarity_scores, human_scores)\n","print(f\"Pearson correlation coefficient: {pearson_coef}, and p-value: {p_pearson}\")\n","\n","spearman_coef, p_spearman = spearmanr(similarity_scores, human_scores)\n","print(f\"Spearman correlation coefficient: {spearman_coef}, and p-value: {p_spearman}\")\n","\n","print()\n","\n","c_pearson_coef, c_p_pearson = pearsonr(c_similarity_scores, human_scores)\n","print(f\"Pearson correlation coefficient for added vectors: {c_pearson_coef}, and p-value: {c_p_pearson}\")\n","\n","c_spearman_coef, c_p_spearman = spearmanr(c_similarity_scores, human_scores)\n","print(f\"Spearman correlation coefficient for added vectors: {c_spearman_coef}, and p-value: {c_p_spearman}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrOEPXsTbOVO"},"outputs":[],"source":["# Load the dataset as a list of word pairs and human similarity scores\n","with open('wordsim_similarity_goldstandard.txt', 'r') as f:\n","    lines = f.readlines()\n","    lines = [line.strip().split() for line in lines]\n","    word_pairs = [(line[0], line[1]) for line in lines]\n","    # WordSim similarities are [0,10], need to normalize down to [-1,1]\n","    human_scores = [(float(line[2]) / 10 * 2 - 1) for line in lines]\n","\n","# Calculate cosine similarity with target embeddings\n","similarity_scores = []\n","for pair in word_pairs:\n","    try:\n","        word1, word2 = pair\n","        similarity = model.cosine_similarity(word1, word2)\n","        similarity_scores.append(similarity)\n","    except KeyError:\n","        similarity_scores.append(-1)\n","\n","# Calculate cosine similarity with context+target embeddings\n","c_similarity_scores = []\n","for pair in word_pairs:\n","    try:\n","        word1, word2 = pair\n","        v1 = model.w(torch.as_tensor(model.voc[word1]))/2 + model.c(torch.as_tensor(model.voc[word1]))/2\n","        v2 = model.w(torch.as_tensor(model.voc[word2]))/2 + model.c(torch.as_tensor(model.voc[word2]))/2\n","        sim = nn.CosineSimilarity(dim=0)\n","        similarity = sim(v1, v2).item()\n","        c_similarity_scores.append(similarity)\n","    except KeyError:\n","        c_similarity_scores.append(-1)\n","\n","\n","pearson_coef, p_pearson = pearsonr(similarity_scores, human_scores)\n","print(f\"Pearson correlation coefficient: {pearson_coef}, and p-value: {p_pearson}\")\n","\n","spearman_coef, p_spearman = spearmanr(similarity_scores, human_scores)\n","print(f\"Spearman correlation coefficient: {spearman_coef}, and p-value: {p_spearman}\")\n","\n","print()\n","\n","c_pearson_coef, c_p_pearson = pearsonr(c_similarity_scores, human_scores)\n","print(f\"Pearson correlation coefficient for added vectors: {c_pearson_coef}, and p-value: {c_p_pearson}\")\n","\n","c_spearman_coef, c_p_spearman = spearmanr(c_similarity_scores, human_scores)\n","print(f\"Spearman correlation coefficient for added vectors: {c_spearman_coef}, and p-value: {c_p_spearman}\")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1O2GNdHxeagcE5fhs-8slS3n9iEzUoFha","timestamp":1680283606049},{"file_id":"1YM_FbkxJqEttTdAPL79I_QMPKbHJUIBX","timestamp":1680255205796},{"file_id":"1_ian039WL__VdYaW6PoOtcgHkdlUlsOJ","timestamp":1680214997466}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}