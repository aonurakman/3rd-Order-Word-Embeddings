{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyO4q2Z1RfMsQM5MNVcde6cG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Experiments/check-standard-models\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-eStMspOzLRb","executionInfo":{"status":"ok","timestamp":1686473278556,"user_tz":-120,"elapsed":37255,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"3fca55dd-6212-4d05-8029-48921e09e79e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Experiments/check-standard-models\n","combined.csv\t\t\t      wordsim_similarity_goldstandard.txt\n","wordsim_relatedness_goldstandard.txt\n"]}]},{"cell_type":"code","source":["import gensim.downloader as api\n","import math\n","from scipy.stats import spearmanr\n","from statistics import mean\n","import csv"],"metadata":{"id":"O5jPCoVJzQ20"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"9RbAz83X8EZI"}},{"cell_type":"markdown","source":["**Word2Vec Google News**: The 'word2vec-google-news-300' is a pre-trained Word2Vec model by Google. It was trained on a part of the Google News dataset, covering approximately 3 million words and phrases. Such a large volume of data enables the capturing of many semantic relationships between words.\n","\n","Word vectors are 300-dimensional."],"metadata":{"id":"AEI3pRXn8j6g"}},{"cell_type":"code","source":["google_model = api.load('word2vec-google-news-300')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzahRVBp8jnU","executionInfo":{"status":"ok","timestamp":1686473735831,"user_tz":-120,"elapsed":455669,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"1f8444a7-0939-4d9e-c40c-0f5d8fcb69fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}]},{"cell_type":"markdown","source":["**FastText:** Developed by Facebook's AI Research lab (FAIR), FastText embeddings are trained on Wikipedia and are unique because they are based on the morphemes of words (subword information) rather than whole words, which allows them to understand the semantics of out-of-vocabulary (OOV) words."],"metadata":{"id":"t8E-2PtK8kbk"}},{"cell_type":"code","source":["fasttext_model = api.load('fasttext-wiki-news-subwords-300')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k8hiiHBq8jgt","executionInfo":{"status":"ok","timestamp":1686474159714,"user_tz":-120,"elapsed":423905,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"6c9f05c2-2c51-428f-a5c2-135ea95f5804"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 958.5/958.4MB downloaded\n"]}]},{"cell_type":"markdown","source":["**ConceptNet Numberbatch:** Numberbatch embeddings combine information from multiple sources, including ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, to create word embeddings that have been demonstrated to perform strongly in bias evaluation and similarity tasks."],"metadata":{"id":"QsVdRDAi8kqw"}},{"cell_type":"code","source":["numberbatch_model = api.load('conceptnet-numberbatch-17-06-300')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xx1v8px18jbH","executionInfo":{"status":"ok","timestamp":1686474798283,"user_tz":-120,"elapsed":638593,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"50ae292f-a931-4794-c7e0-f89067783d6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1168.7/1168.7MB downloaded\n"]}]},{"cell_type":"markdown","source":["**GloVe:** Global Vectors for Word Representation. These embeddings were trained on various corpora (Wikipedia 2014 + Gigaword 5, Common Crawl, and Twitter) by the Stanford NLP Group. They apply a global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods."],"metadata":{"id":"XCzGZDY48k_I"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Q_pzBV3zGag","executionInfo":{"status":"ok","timestamp":1686474965463,"user_tz":-120,"elapsed":167199,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"de8b3cf3-fffc-46dd-d2bf-ef113e1ac352"},"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 376.1/376.1MB downloaded\n"]}],"source":["glove_model = api.load('glove-wiki-gigaword-300')"]},{"cell_type":"code","source":["models = {\"Word2Vec-Google-News\": google_model, \"FastText\": fasttext_model, \"ConceptNet-Numberbatch\": numberbatch_model, \"GloVe\": glove_model}"],"metadata":{"id":"77sbxcYc9Qdt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"hD6Hv6qr9KDJ"}},{"cell_type":"code","source":["def cosine_similarity(v1, v2):\n","    sumxx, sumxy, sumyy = 0, 0, 0\n","    for i in range(len(v1)):\n","        x = v1[i];\n","        y = v2[i]\n","        sumxx += x * x\n","        sumyy += y * y\n","        sumxy += x * y\n","    return sumxy / math.sqrt(sumxx * sumyy)"],"metadata":{"id":"RibvzUg_0y6F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scorefunction1(embed, specify_lang):\n","\n","  with open('combined.csv') as csvfile:\n","    filein = csv.reader(csvfile)\n","\n","    consim = []\n","    humansim = []\n","\n","    inv_line = True\n","\n","    for eles in filein:\n","\n","      if inv_line:\n","        inv_line = False\n","        continue\n","\n","      word1 = eles[0]\n","      word2 = eles[1]\n","      humansim.append(float(eles[2]) / 10 * 2 - 1)\n","\n","      try:\n","        if specify_lang:\n","          word1 = \"/c/en/\" + word1\n","          word2 = \"/c/en/\" + word2\n","        value1 = embed[word1]\n","        value2 = embed[word2]\n","        score = cosine_similarity(value1, value2)\n","        consim.append(score)\n","      except KeyError:\n","        consim.append(-1)\n","\n","  cor1, pvalue1 = spearmanr(humansim, consim)\n","\n","  return cor1"],"metadata":{"id":"axpPoGrKzY88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scorefunction2(embed, specify_lang):\n","  \n","  lines = open('wordsim_similarity_goldstandard.txt', 'r').readlines()\n","\n","  consim = []\n","  humansim = []\n","\n","  for line in lines:\n","    eles = line.strip().split()\n","    word1 = eles[0]\n","    word2 = eles[1]\n","    humansim.append(float(eles[2]) / 10 * 2 - 1)\n","\n","    try:\n","      if specify_lang:\n","        word1 = \"/c/en/\" + word1\n","        word2 = \"/c/en/\" + word2\n","      value1 = embed[word1]\n","      value2 = embed[word2]\n","      score = cosine_similarity(value1, value2)\n","      consim.append(score)\n","    except KeyError:\n","      consim.append(-1)\n","\n","  cor2, pvalue2 = spearmanr(humansim, consim)\n","\n","  return cor2"],"metadata":{"id":"DfK7bnW3zfWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scorefunction3(embed, specify_lang):\n","\n","  lines = open('wordsim_relatedness_goldstandard.txt', 'r').readlines()\n","\n","  consim = []\n","  humansim = []\n","\n","  for line in lines:\n","    eles = line.strip().split()\n","\n","    word1 = eles[0]\n","    word2 = eles[1]\n","    humansim.append(float(eles[2]) / 10 * 2 - 1)\n","\n","    try:\n","      if specify_lang:\n","        word1 = \"/c/en/\" + word1\n","        word2 = \"/c/en/\" + word2\n","      value1 = embed[word1]\n","      value2 = embed[word2]\n","      score = cosine_similarity(value1, value2)\n","      consim.append(score)\n","    except KeyError:\n","      consim.append(-1)\n","\n","  cor3, pvalue3 = spearmanr(humansim, consim)\n","\n","  return cor3"],"metadata":{"id":"jpWXXpKgzhaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_scores(word_embeddings, model_name):\n","\n","  specify_lang = (model_name == \"ConceptNet-Numberbatch\")\n","\n","\n","  sp1 = scorefunction1(word_embeddings, specify_lang)\n","  sp2 = scorefunction2(word_embeddings, specify_lang)\n","  sp3 = scorefunction3(word_embeddings, specify_lang)\n","\n","  return (sp1, sp2, sp3, mean([sp1, sp2, sp3]))"],"metadata":{"id":"inzpZyijzi7q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"1R71i4Yc9lwm"}},{"cell_type":"code","source":["for model_name in list(models.keys()):\n","\n","  sp1, sp2, sp3, avg = get_scores(models[model_name], model_name)\n","\n","  print(\"Model: %s\" % (model_name))\n","  print(\"Correlation with Wordsim353: %.4f\" % (sp1))\n","  print(\"Correlation with Wordsim Similarity Goldstandard: %.4f\" % (sp2))\n","  print(\"Correlation with Wordsim Relatedness Goldstandard: %.4f\" % (sp3))\n","  print(\"Average of all: %.4f\" % (avg))\n","  print(\"----------\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_Cgn1b39Mqa","executionInfo":{"status":"ok","timestamp":1686476225043,"user_tz":-120,"elapsed":1085,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"364bc25f-6b57-42ca-a87c-422b8c2496fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: Word2Vec-Google-News\n","Correlation with Wordsim353: 0.7000\n","Correlation with Wordsim Similarity Goldstandard: 0.7717\n","Correlation with Wordsim Relatedness Goldstandard: 0.6355\n","Average of all: 0.7024\n","----------\n","Model: FastText\n","Correlation with Wordsim353: 0.6943\n","Correlation with Wordsim Similarity Goldstandard: 0.8235\n","Correlation with Wordsim Relatedness Goldstandard: 0.6206\n","Average of all: 0.7128\n","----------\n","Model: ConceptNet-Numberbatch\n","Correlation with Wordsim353: 0.7040\n","Correlation with Wordsim Similarity Goldstandard: 0.8015\n","Correlation with Wordsim Relatedness Goldstandard: 0.6109\n","Average of all: 0.7054\n","----------\n","Model: GloVe\n","Correlation with Wordsim353: 0.5109\n","Correlation with Wordsim Similarity Goldstandard: 0.6387\n","Correlation with Wordsim Relatedness Goldstandard: 0.4358\n","Average of all: 0.5285\n","----------\n"]}]},{"cell_type":"markdown","source":["# Save models"],"metadata":{"id":"QDH9M6rlFbxL"}},{"cell_type":"code","source":["for model_name in list(models.keys()):\n","  models[model_name].save(\"models/%s.model\" % (model_name.replace(\" \", \"-\")))"],"metadata":{"id":"ilHoTAGrFeHw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Disconnect from the runtime"],"metadata":{"id":"DRbgcugUk76l"}},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"170FaZjrk-f1"},"execution_count":null,"outputs":[]}]}