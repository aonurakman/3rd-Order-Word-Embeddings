{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm","authorship_tag":"ABX9TyMVWxj8uILJAs4eIcuWiwi2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","from prettytable import PrettyTable"],"metadata":{"id":"Tv_dyartTuSa","executionInfo":{"status":"ok","timestamp":1685450300506,"user_tz":-120,"elapsed":4045,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Mount"],"metadata":{"id":"T1PzAf5eTkzz"}},{"cell_type":"code","source":["drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Experiments/Galvan-MyVersion-Test\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDoNhsW-TlPh","executionInfo":{"status":"ok","timestamp":1685450331786,"user_tz":-120,"elapsed":31285,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"a644d147-3ce2-4e4e-fc38-054e771865ee"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Experiments/Galvan-MyVersion-Test\n","max_score_model.torch  wordsim_relatedness_goldstandard.txt\n","min_loss_model.torch   wordsim_similarity_goldstandard.txt\n","voca.txt\n"]}]},{"cell_type":"markdown","source":["# Classes for the model"],"metadata":{"id":"XGe4zz03QAP4"}},{"cell_type":"markdown","source":["## Galvan Model Class"],"metadata":{"id":"aLjQSA-VQGY5"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"Oh8F-a1JNKY3","executionInfo":{"status":"ok","timestamp":1685450331786,"user_tz":-120,"elapsed":5,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"outputs":[],"source":["class Modello(nn.Module):\n","    def __init__(self, n_vocab, n_embed):\n","        super().__init__()\n","\n","        self.n_vocab = n_vocab\n","        self.n_embed = n_embed\n","\n","        self.in_embed = nn.Embedding(n_vocab, n_embed, dtype=torch.float64)\n","        self.out_embed = nn.Embedding(n_vocab, n_embed, dtype=torch.float64)\n","        # initrange = 0.5 / n_embed\n","        self.in_embed.weight.data.uniform_(-1, 1)\n","        self.out_embed.weight.data.uniform_(-1, 1)\n","\n","    def forward_input(self, input_words): # takes a batch of input words and returns their embeddings.\n","        input_vector = self.in_embed(input_words)\n","        return input_vector\n","\n","    def forward_output(self, output_words1, output_words2): # takes two batches of output words and returns their embeddings.\n","        output_vector1 = self.out_embed(output_words1)\n","        output_vector2 = self.out_embed(output_words2)\n","        return output_vector1, output_vector2\n","\n","    def input_embeddings(self): # returns the input embeddings as a numpy array\n","        return self.in_embed.weight.data.cpu().numpy()\n","\n","    def embeddinginput_dictionary(self, id2word): # return dictionary that map words to their corresponding input embeddings\n","        embedding = self.in_embed.weight.cpu().data.numpy()\n","        E = {}\n","        for wid, w in id2word.items():\n","            E[w] = embedding[wid]\n","        return E\n","\n","    def embeddingoutput_dictionary(self, id2word): # return dictionary that map words to their corresponding input embeddings\n","        embedding = self.out_embed.weight.cpu().data.numpy()\n","        E = {}\n","        for wid, w in id2word.items():\n","            E[w] = embedding[wid]\n","        return E\n","\n","    def forward_noise(self, noise_words): # takes a batch of noise words and returns their embeddings\n","        noise_vector = self.out_embed(noise_words)\n","        return noise_vector\n"]},{"cell_type":"markdown","source":["# Reading Words"],"metadata":{"id":"z3XrB9_qRiu8"}},{"cell_type":"markdown","source":["## Galvan"],"metadata":{"id":"c3BEx-rhRl88"}},{"cell_type":"code","source":["f = open('voca.txt')\n","line = f.readline()\n","\n","galvan_vocab = []\n","galvan_wordindex = dict()\n","index = 0\n","\n","while line:\n","    word = line.strip().split()[0]\n","    galvan_vocab.append(word)\n","    galvan_wordindex[word] = index\n","    index = index + 1\n","    line = f.readline()\n","\n","f.close()"],"metadata":{"id":"w3ZdHC5xRlEM","executionInfo":{"status":"ok","timestamp":1685450332306,"user_tz":-120,"elapsed":524,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Getting Models"],"metadata":{"id":"_wLodTWuSJjS"}},{"cell_type":"markdown","source":["## Galvan"],"metadata":{"id":"ph8G4fO6SLI2"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","embedding_dim = 300\n","\n","galvan_model_score = Modello(63503, embedding_dim).to(device)\n","galvan_model_loss = Modello(63503, embedding_dim).to(device)"],"metadata":{"id":"wlbw09tFSM8-","executionInfo":{"status":"ok","timestamp":1685450334878,"user_tz":-120,"elapsed":2575,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["galvan_model_score.load_state_dict(torch.load(\"max_score_model.torch\", map_location=device))\n","galvan_model_loss.load_state_dict(torch.load(\"min_loss_model.torch\", map_location=device))\n","galvan_model_loss.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p34DPnqKSbia","executionInfo":{"status":"ok","timestamp":1685450354573,"user_tz":-120,"elapsed":19698,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"33bb869b-8346-453d-e4b3-f26ea0896bae"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Modello(\n","  (in_embed): Embedding(63503, 300)\n","  (out_embed): Embedding(63503, 300)\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["galvan_score_embed = galvan_model_score.input_embeddings()\n","galvan_loss_embed = galvan_model_loss.input_embeddings()"],"metadata":{"id":"sdWQHAYeSo6U","executionInfo":{"status":"ok","timestamp":1685450354574,"user_tz":-120,"elapsed":28,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Get embedding from given word and vice versa\n","\n","def galvan_score_emb_from_word(word):\n","  try:\n","    return galvan_score_embed[int(galvan_wordindex[word])]\n","  except KeyError:\n","    return None\n","\n","def galvan_score_word_from_embed(embed):\n","  try:\n","    ex_idx = next((idx for idx, emb in enumerate(galvan_score_embed) if np.array_equal(emb, embed)), None)\n","    return next((key for key, value in galvan_wordindex.items() if value == ex_idx), None)\n","  except KeyError:\n","    return None\n"],"metadata":{"id":"bmEJYbDtLkm6","executionInfo":{"status":"ok","timestamp":1685450354575,"user_tz":-120,"elapsed":28,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(galvan_score_word_from_embed(galvan_score_emb_from_word(\"bread\")))\n","print(galvan_score_word_from_embed(galvan_score_emb_from_word(\"tunusia\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hv7o_jKXMBFN","executionInfo":{"status":"ok","timestamp":1685450354575,"user_tz":-120,"elapsed":27,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"ff63146d-c402-475d-c0d2-83a9f8ac269c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["bread\n","None\n"]}]},{"cell_type":"code","source":["# Get embedding from given word and vice versa\n","\n","def galvan_loss_emb_from_word(word):\n","  try:\n","    return galvan_loss_embed[int(galvan_wordindex[word])]\n","  except KeyError:\n","    return None\n","\n","def galvan_loss_word_from_embed(embed):\n","  try:\n","    ex_idx = next((idx for idx, emb in enumerate(galvan_loss_embed) if np.array_equal(emb, embed)), None)\n","    return next((key for key, value in galvan_wordindex.items() if value == ex_idx), None)\n","  except KeyError:\n","    return None"],"metadata":{"id":"Acz7oBeWMmpL","executionInfo":{"status":"ok","timestamp":1685450354576,"user_tz":-120,"elapsed":24,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(galvan_loss_word_from_embed(galvan_loss_emb_from_word(\"bread\")))\n","print(galvan_loss_word_from_embed(galvan_loss_emb_from_word(\"tunusia\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjMcSl3SMmiD","executionInfo":{"status":"ok","timestamp":1685450354577,"user_tz":-120,"elapsed":24,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"c22d708f-318b-4d11-d413-2dcca9c077da"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["bread\n","None\n"]}]},{"cell_type":"markdown","source":["# Standards"],"metadata":{"id":"6MReDps8T8eb"}},{"cell_type":"code","source":["similarity_standard = dict()\n","\n","with open('wordsim_similarity_goldstandard.txt', 'r') as f:\n","    lines = f.readlines()\n","    lines = [line.strip().split() for line in lines]\n","\n","    similarity_word_pairs = [(line[0], line[1]) for line in lines]\n","    similarity_human_scores = [float(line[2]) for line in lines] #  / 10 * 2 - 1\n","\n","    similarity_word_pairs.extend([(line[1], line[0]) for line in lines])\n","    similarity_human_scores.extend([float(line[2]) for line in lines])\n","    \n","    similarity_voc = {pair[0] for pair in similarity_word_pairs}\n","\n","    for x in range(len(similarity_human_scores)):\n","      similarity_standard[similarity_word_pairs[x]] = similarity_human_scores[x]\n"],"metadata":{"id":"8AKrwbcwT_kv","executionInfo":{"status":"ok","timestamp":1685450354577,"user_tz":-120,"elapsed":19,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["similarity_standard[('school','center')]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsR-0DaNVyuZ","executionInfo":{"status":"ok","timestamp":1685450354578,"user_tz":-120,"elapsed":19,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"5123b163-f51d-44ee-b7b0-931d52998cd0"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.44"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["relatedness_standard = dict()\n","\n","with open('wordsim_relatedness_goldstandard.txt', 'r') as f:\n","    lines = f.readlines()\n","    lines = [line.strip().split() for line in lines]\n","\n","    relatedness_word_pairs = [(line[0], line[1]) for line in lines]\n","    relatedness_human_scores = [float(line[2]) for line in lines] #  / 10 * 2 - 1\n","\n","    relatedness_word_pairs.extend([(line[1], line[0]) for line in lines])\n","    relatedness_human_scores.extend([float(line[2]) for line in lines])\n","    \n","    relatedness_voc = {pair[0] for pair in relatedness_word_pairs}\n","\n","    for x in range(len(relatedness_human_scores)):\n","      relatedness_standard[relatedness_word_pairs[x]] = relatedness_human_scores[x]"],"metadata":{"id":"xXR5HstcUbJN","executionInfo":{"status":"ok","timestamp":1685450355159,"user_tz":-120,"elapsed":596,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["relatedness_standard[('morality', 'marriage')]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZagbH7sWqnE","executionInfo":{"status":"ok","timestamp":1685450355159,"user_tz":-120,"elapsed":21,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"5c329c9f-c089-42ed-82e1-b96a6d8f2e42"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.69"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Similarity Function"],"metadata":{"id":"b779___AKsXQ"}},{"cell_type":"code","source":["def cosine_similarity(v1, v2):\n","    dot_product = np.dot(v1, v2)\n","    \n","    norm_v1 = np.linalg.norm(v1)\n","    \n","    norm_v2 = np.linalg.norm(v2)\n","    \n","    cos_sim = dot_product / (norm_v1 * norm_v2)\n","    \n","    return cos_sim"],"metadata":{"id":"b0t6PN5gPlI1","executionInfo":{"status":"ok","timestamp":1685450355160,"user_tz":-120,"elapsed":21,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(cosine_similarity(galvan_loss_emb_from_word(\"school\"), galvan_loss_emb_from_word(\"student\")))\n","print(cosine_similarity(galvan_score_emb_from_word(\"school\"), galvan_score_emb_from_word(\"student\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kpsG7-WVPmIr","executionInfo":{"status":"ok","timestamp":1685450355160,"user_tz":-120,"elapsed":21,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"cc375e6e-6a45-417b-d932-5d28fe94fda7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["-0.06073248893174742\n","-0.06073248893174742\n"]}]},{"cell_type":"markdown","source":["# Nearest Neighbors"],"metadata":{"id":"eAAiWef8QJHc"}},{"cell_type":"markdown","source":["## Galvan"],"metadata":{"id":"mspoJ3IdWTvC"}},{"cell_type":"code","source":["def galvan_neighbors_from_word(word, model = \"score\", topk = 10):\n","\n","  neighbor_words = [\"<NULL>\"] * topk\n","  neighbor_similarities = [-1.0] * topk\n","  min_neighbor = neighbor_similarities.index(min(neighbor_similarities))\n","\n","  if word not in galvan_wordindex.keys():\n","    return neighbor_words, neighbor_similarities\n","\n","  word2emb = galvan_score_emb_from_word\n","\n","  if model == \"loss\":\n","    word2emb = galvan_loss_emb_from_word\n","\n","  word_vector = word2emb(word)\n","\n","  for w in galvan_wordindex.keys():\n","    emb = word2emb(w)\n","    sim = cosine_similarity(word_vector, emb)\n","\n","    if sim >= neighbor_similarities[min_neighbor]:\n","      neighbor_similarities[min_neighbor] = sim\n","      neighbor_words[min_neighbor] = w\n","\n","      min_neighbor = neighbor_similarities.index(min(neighbor_similarities))\n","\n","  return neighbor_words, neighbor_similarities  "],"metadata":{"id":"mlCGkC7zQIrI","executionInfo":{"status":"ok","timestamp":1685450400527,"user_tz":-120,"elapsed":348,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["print(galvan_neighbors_from_word(\"car\", \"loss\", 5)[0])\n","print(galvan_neighbors_from_word(\"car\", \"score\", 5)[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HgHHP9NPTVt-","executionInfo":{"status":"ok","timestamp":1685450402400,"user_tz":-120,"elapsed":1043,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"df92dc03-62d1-459b-fe7e-3a5f69d367c4"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["['rectum', 'angered', 'viewpoint', 'car', 'monuc']\n","['rectum', 'angered', 'viewpoint', 'car', 'monuc']\n"]}]},{"cell_type":"markdown","source":["## Standards"],"metadata":{"id":"B0HVzaWDWYkB"}},{"cell_type":"code","source":["def standards_neighbors_from_word(word, std_type = \"similarity\", topk = 10):\n","  std = similarity_standard\n","  std_voc = similarity_voc\n","\n","  if std_type == \"relatedness\":\n","    std_voc = relatedness_voc\n","    std = relatedness_standard\n","\n","  neighbor_words = [\"<NULL>\"] * topk\n","  neighbor_similarities = [0.0] * topk\n","  min_neighbor = neighbor_similarities.index(min(neighbor_similarities))\n","\n","  if word not in std_voc:\n","    return neighbor_words, neighbor_similarities\n","\n","  similarities = [(pair[1], std[pair]) for pair in std.keys() if pair[0] == word]\n","\n","  for s in similarities:\n","    w, sim = s\n","\n","    if sim >= neighbor_similarities[min_neighbor]:\n","      neighbor_similarities[min_neighbor] = sim\n","      neighbor_words[min_neighbor] = w\n","\n","      min_neighbor = neighbor_similarities.index(min(neighbor_similarities))\n","\n","  return neighbor_words, neighbor_similarities"],"metadata":{"id":"q6ZXGWnMWbAG","executionInfo":{"status":"ok","timestamp":1685450402400,"user_tz":-120,"elapsed":11,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["print(standards_neighbors_from_word(\"car\", \"similarity\"))\n","print(standards_neighbors_from_word(\"car\", \"relatedness\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnkwyrGPajAM","executionInfo":{"status":"ok","timestamp":1685450402400,"user_tz":-120,"elapsed":10,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"f73a9927-f4e1-4d82-ad6e-f0366821ad40"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["(['automobile', 'flight', 'plane', 'train', 'jaguar', 'drink', '<NULL>', '<NULL>', '<NULL>', '<NULL>'], [8.94, 4.94, 5.77, 6.31, 7.27, 3.04, 0.0, 0.0, 0.0, 0.0])\n","(['flight', 'luxury', 'journey', 'drink', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>', '<NULL>'], [4.94, 6.47, 5.85, 3.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n"]}]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"Egr0B1gidWiG"}},{"cell_type":"code","source":["def neighbors_table(word, topk = 10, print_scores = False):\n","  print(\"\\n\\nFor given word: \", word, \"\\n\")\n","\n","  data = []\n","  data.append(standards_neighbors_from_word(word, \"similarity\", topk))\n","  data.append(standards_neighbors_from_word(word, \"relatedness\", topk))\n","  data.append(galvan_neighbors_from_word(word, \"loss\", topk))\n","  data.append(galvan_neighbors_from_word(word, \"score\", topk))\n","\n","  table = PrettyTable()\n","  table.add_column(\"Similarity Standard\", data[0][0])\n","  table.add_column(\"Relatedness Standard\", data[1][0])\n","  table.add_column(\"Galvan Min Loss\", data[2][0])\n","  table.add_column(\"Galvan Max Score\", data[3][0])\n","\n","  print(\"--- NEIGHBORS ---\")   \n","  print(table)\n","\n","\n","  table2 = PrettyTable()\n","  table2.add_column(\"Similarity Standard\", data[0][1])\n","  table2.add_column(\"Relatedness Standard\", data[1][1])\n","  table2.add_column(\"Galvan Min Loss\", data[2][1])\n","  table2.add_column(\"Galvan Max Score\", data[3][1])\n","\n","  if print_scores:\n","    print(\"\\n\\n--- SIMILARITIES ---\")\n","    print(table2)"],"metadata":{"id":"OJj9SW4ukKbn","executionInfo":{"status":"ok","timestamp":1685450402401,"user_tz":-120,"elapsed":9,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","def most_frequent_items(lst1, lst2, n):\n","    counter = Counter(lst1)\n","    counter.update(lst2)\n","    most_common = counter.most_common(n)\n","    return [item for item, count in most_common]\n","\n","lst1 = [pair[0] for pair in similarity_word_pairs]\n","lst2 = [pair[0] for pair in relatedness_word_pairs]\n","common = most_frequent_items(lst1, lst2, 5)\n","\n","print(\"Most common words in standards: \", common)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oBrSjiCn4eS","executionInfo":{"status":"ok","timestamp":1685450402401,"user_tz":-120,"elapsed":9,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"8d8a8495-dd4b-4855-bc67-a5764f66c92c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Most common words in standards:  ['stock', 'money', 'cup', 'psychology', 'tiger']\n"]}]},{"cell_type":"code","source":["topk = 5\n","print_scores = False\n","\n","for word in common:\n","  neighbors_table(word, topk, print_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rF69KIoBdX8Y","executionInfo":{"status":"ok","timestamp":1685450407525,"user_tz":-120,"elapsed":5130,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"d3c9e715-eaf6-469e-e9f3-7b0eadfd806c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","For given word:  stock \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|         live        |        market        |      stock      |      stock       |\n","|         egg         |         live         |    animation    |    animation     |\n","|        phone        |         egg          |  socialization  |  socialization   |\n","|          CD         |         oil          |      franjo     |      franjo      |\n","|        jaguar       |       company        |   standardised  |   standardised   |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","For given word:  money \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|         cash        |         bank         |     curtail     |     curtail      |\n","|        dollar       |        wealth        |     cognates    |     cognates     |\n","|       currency      |       deposit        |      money      |      money       |\n","|      operation      |       property       |      codify     |      codify      |\n","|        <NULL>       |      possession      |     unabated    |     unabated     |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","For given word:  cup \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|      tableware      |        drink         |    macdonald    |    macdonald     |\n","|       artifact      |        coffee        |    kinematic    |    kinematic     |\n","|        object       |        liquid        |       bin       |       bin        |\n","|       article       |         food         |       cup       |       cup        |\n","|         food        |       article        |     cepheus     |     cepheus      |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","For given word:  psychology \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|      psychiatry     |        Freud         |     rudyard     |     rudyard      |\n","|       science       |         mind         |       lcao      |       lcao       |\n","|      discipline     |      cognition       |    psychology   |    psychology    |\n","|        <NULL>       |      depression      |     wagering    |     wagering     |\n","|        <NULL>       |        health        |     rhoades     |     rhoades      |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","For given word:  tiger \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-------------------+-------------------+\n","| Similarity Standard | Relatedness Standard |  Galvan Min Loss  |  Galvan Max Score |\n","+---------------------+----------------------+-------------------+-------------------+\n","|         cat         |         zoo          |      entrance     |      entrance     |\n","|        tiger        |        <NULL>        |    implemented    |    implemented    |\n","|        jaguar       |        <NULL>        |       tiger       |       tiger       |\n","|        feline       |        <NULL>        | misidentification | misidentification |\n","|      carnivore      |        <NULL>        |     concertos     |     concertos     |\n","+---------------------+----------------------+-------------------+-------------------+\n"]}]},{"cell_type":"code","source":["word = \"car\"\n","topk = 10\n","print_scores = True\n","\n","neighbors_table(word, topk, print_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZpdaU-ylKHJ","executionInfo":{"status":"ok","timestamp":1685450408798,"user_tz":-120,"elapsed":1280,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"bc49118e-3d98-4498-ebe8-090b607926d0"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","For given word:  car \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|      automobile     |        flight        |      monuc      |      monuc       |\n","|        flight       |        luxury        |      rectum     |      rectum      |\n","|        plane        |       journey        |       car       |       car        |\n","|        train        |        drink         |     robbers     |     robbers      |\n","|        jaguar       |        <NULL>        |     angered     |     angered      |\n","|        drink        |        <NULL>        |       cong      |       cong       |\n","|        <NULL>       |        <NULL>        |    viewpoint    |    viewpoint     |\n","|        <NULL>       |        <NULL>        |     classics    |     classics     |\n","|        <NULL>       |        <NULL>        |     amaranth    |     amaranth     |\n","|        <NULL>       |        <NULL>        |     arrange     |     arrange      |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","--- SIMILARITIES ---\n","+---------------------+----------------------+---------------------+---------------------+\n","| Similarity Standard | Relatedness Standard |   Galvan Min Loss   |   Galvan Max Score  |\n","+---------------------+----------------------+---------------------+---------------------+\n","|         8.94        |         4.94         | 0.22830130083140815 | 0.22830130083140815 |\n","|         4.94        |         6.47         | 0.24243931614431732 | 0.24243931614431732 |\n","|         5.77        |         5.85         |  0.9999999999999999 |  0.9999999999999999 |\n","|         6.31        |         3.04         | 0.21752811179738665 | 0.21752811179738665 |\n","|         7.27        |         0.0          | 0.24625500856185792 | 0.24625500856185792 |\n","|         3.04        |         0.0          |  0.2231227771942488 |  0.2231227771942488 |\n","|         0.0         |         0.0          | 0.25665089543284714 | 0.25665089543284714 |\n","|         0.0         |         0.0          | 0.22073730944061387 | 0.22073730944061387 |\n","|         0.0         |         0.0          |  0.2214968112580734 |  0.2214968112580734 |\n","|         0.0         |         0.0          |  0.217653012195526  |  0.217653012195526  |\n","+---------------------+----------------------+---------------------+---------------------+\n"]}]},{"cell_type":"markdown","source":["# Phase 2"],"metadata":{"id":"GOA-EZFjrO41"}},{"cell_type":"markdown","source":["# Context Embeddings"],"metadata":{"id":"tcMDC4A7rY8t"}},{"cell_type":"code","source":["galvan_score_context = galvan_model_score.out_embed.weight.data.cpu().numpy()\n","galvan_loss_context = galvan_model_loss.out_embed.weight.data.cpu().numpy()"],"metadata":{"id":"skzUVQDZrTY2","executionInfo":{"status":"ok","timestamp":1685450408798,"user_tz":-120,"elapsed":11,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"UtiHPFnytUX9"}},{"cell_type":"code","source":["def galvan_loss_emb_from_word2(word):\n","  try:\n","    return galvan_loss_context[int(galvan_wordindex[word])]\n","  except KeyError:\n","    return None\n","\n","def galvan_loss_word_from_embed2(embed):\n","  try:\n","    ex_idx = next((idx for idx, emb in enumerate(galvan_loss_context) if np.array_equal(emb, embed)), None)\n","    return next((key for key, value in galvan_wordindex.items() if value == ex_idx), None)\n","  except KeyError:\n","    return None"],"metadata":{"id":"fwuBQn19sHO7","executionInfo":{"status":"ok","timestamp":1685450408799,"user_tz":-120,"elapsed":11,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def galvan_score_emb_from_word2(word):\n","  try:\n","    return galvan_score_context[int(galvan_wordindex[word])]\n","  except KeyError:\n","    return None\n","\n","def galvan_score_word_from_embed2(embed):\n","  try:\n","    ex_idx = next((idx for idx, emb in enumerate(galvan_score_context) if np.array_equal(emb, embed)), None)\n","    return next((key for key, value in galvan_wordindex.items() if value == ex_idx), None)\n","  except KeyError:\n","    return None\n"],"metadata":{"id":"gOtdmr_etCpD","executionInfo":{"status":"ok","timestamp":1685450408799,"user_tz":-120,"elapsed":10,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# Neighbor Functions"],"metadata":{"id":"-7w3XOqrtk8n"}},{"cell_type":"code","source":["def galvan_neighbors_from_word2(word, model = \"score\", topk = 10):\n","\n","  neighbor_words = [\"<NULL>\"] * topk\n","  neighbor_similarities = [-1.0] * topk\n","  min_neighbor = neighbor_similarities.index(min(neighbor_similarities))\n","\n","  if word not in galvan_wordindex.keys():\n","    return neighbor_words, neighbor_similarities\n","\n","  word2emb = galvan_score_emb_from_word2\n","\n","  if model == \"loss\":\n","    word2emb = galvan_loss_emb_from_word2\n","\n","  word_vector = word2emb(word)\n","\n","  for w in galvan_wordindex.keys():\n","    emb = word2emb(w)\n","    sim = cosine_similarity(word_vector, emb)\n","\n","    if sim >= neighbor_similarities[min_neighbor]:\n","      neighbor_similarities[min_neighbor] = sim\n","      neighbor_words[min_neighbor] = w\n","\n","      min_neighbor = neighbor_similarities.index(min(neighbor_similarities))\n","\n","  return neighbor_words, neighbor_similarities "],"metadata":{"id":"mS034VqLtlUv","executionInfo":{"status":"ok","timestamp":1685450408799,"user_tz":-120,"elapsed":10,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"OGCOGep7uLe9"}},{"cell_type":"code","source":["def neighbors_table2(word, topk = 10, print_scores = False):\n","  print(\"\\n\\nFor given word: \", word, \"\\n\")\n","\n","  data = []\n","  data.append(standards_neighbors_from_word(word, \"similarity\", topk))\n","  data.append(standards_neighbors_from_word(word, \"relatedness\", topk))\n","  data.append(galvan_neighbors_from_word2(word, \"loss\", topk))\n","  data.append(galvan_neighbors_from_word2(word, \"score\", topk))\n","\n","  table = PrettyTable()\n","  table.add_column(\"Similarity Standard\", data[0][0])\n","  table.add_column(\"Relatedness Standard\", data[1][0])\n","  table.add_column(\"Galvan Min Loss\", data[2][0])\n","  table.add_column(\"Galvan Max Score\", data[3][0])\n","\n","  print(\"--- NEIGHBORS ---\")   \n","  print(table)\n","\n","\n","  table2 = PrettyTable()\n","  table2.add_column(\"Similarity Standard\", data[0][1])\n","  table2.add_column(\"Relatedness Standard\", data[1][1])\n","  table2.add_column(\"Galvan Min Loss\", data[2][1])\n","  table2.add_column(\"Galvan Max Score\", data[3][1])\n","\n","  if print_scores:\n","    print(\"\\n\\n--- SIMILARITIES ---\")\n","    print(table2)"],"metadata":{"id":"LiDhFqCyuL3H","executionInfo":{"status":"ok","timestamp":1685450408799,"user_tz":-120,"elapsed":9,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["topk = 5\n","print_scores = False\n","\n","for word in common:\n","  neighbors_table2(word, topk, print_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uUUDFCe1ubjE","executionInfo":{"status":"ok","timestamp":1685450413870,"user_tz":-120,"elapsed":5079,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"4009b2b4-754f-4d5b-9944-036fa1ad902d"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","For given word:  stock \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|         live        |        market        |      ravens     |      ravens      |\n","|         egg         |         live         |    subspecies   |    subspecies    |\n","|        phone        |         egg          |      corso      |      corso       |\n","|          CD         |         oil          |      stock      |      stock       |\n","|        jaguar       |       company        |       dump      |       dump       |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","For given word:  money \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|         cash        |         bank         |    migration    |    migration     |\n","|        dollar       |        wealth        |      money      |      money       |\n","|       currency      |       deposit        |      alpaca     |      alpaca      |\n","|      operation      |       property       |     finances    |     finances     |\n","|        <NULL>       |      possession      |      missal     |      missal      |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","For given word:  cup \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|      tableware      |        drink         |     rampage     |     rampage      |\n","|       artifact      |        coffee        |      hetzel     |      hetzel      |\n","|        object       |        liquid        |    perplexed    |    perplexed     |\n","|       article       |         food         |       cup       |       cup        |\n","|         food        |       article        |    increment    |    increment     |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","For given word:  psychology \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|      psychiatry     |        Freud         |      exons      |      exons       |\n","|       science       |         mind         |      matre      |      matre       |\n","|      discipline     |      cognition       |     fulness     |     fulness      |\n","|        <NULL>       |      depression      |    extremely    |    extremely     |\n","|        <NULL>       |        health        |    psychology   |    psychology    |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","For given word:  tiger \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|         cat         |         zoo          |      tiger      |      tiger       |\n","|        tiger        |        <NULL>        |     paraded     |     paraded      |\n","|        jaguar       |        <NULL>        |     webpages    |     webpages     |\n","|        feline       |        <NULL>        |    symbionese   |    symbionese    |\n","|      carnivore      |        <NULL>        |     shunning    |     shunning     |\n","+---------------------+----------------------+-----------------+------------------+\n"]}]},{"cell_type":"code","source":["word = \"tango\"\n","topk = 3\n","print_scores = True\n","\n","neighbors_table2(word, topk, print_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PlhJqjb1v0Dn","executionInfo":{"status":"ok","timestamp":1685450415083,"user_tz":-120,"elapsed":1219,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"7af78cdb-d0fd-4606-aa5a-62ae7b01a71b"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","For given word:  tango \n","\n","--- NEIGHBORS ---\n","+---------------------+----------------------+-----------------+------------------+\n","| Similarity Standard | Relatedness Standard | Galvan Min Loss | Galvan Max Score |\n","+---------------------+----------------------+-----------------+------------------+\n","|        <NULL>       |        <NULL>        |     arrakis     |     arrakis      |\n","|        <NULL>       |        <NULL>        |     layouts     |     layouts      |\n","|        <NULL>       |        <NULL>        |      tango      |      tango       |\n","+---------------------+----------------------+-----------------+------------------+\n","\n","\n","--- SIMILARITIES ---\n","+---------------------+----------------------+---------------------+---------------------+\n","| Similarity Standard | Relatedness Standard |   Galvan Min Loss   |   Galvan Max Score  |\n","+---------------------+----------------------+---------------------+---------------------+\n","|         0.0         |         0.0          |  0.2541713391603402 |  0.2541713391603402 |\n","|         0.0         |         0.0          | 0.24715244207499926 | 0.24715244207499926 |\n","|         0.0         |         0.0          |  1.0000000000000002 |  1.0000000000000002 |\n","+---------------------+----------------------+---------------------+---------------------+\n"]}]},{"cell_type":"markdown","source":["# Phase 3"],"metadata":{"id":"13_rEOW8wKgA"}},{"cell_type":"code","source":["from statistics import mean "],"metadata":{"id":"LKzNSkOfymxh","executionInfo":{"status":"ok","timestamp":1685450415083,"user_tz":-120,"elapsed":3,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["galvan_loss_ct = []\n","galvan_score_ct = []\n","sgns_ct = []\n","\n","for word in galvan_wordindex.keys():\n","  galvan_loss_ct.append(cosine_similarity(galvan_loss_emb_from_word(word), galvan_loss_emb_from_word2(word)))\n","  galvan_score_ct.append(cosine_similarity(galvan_score_emb_from_word(word), galvan_score_emb_from_word2(word)))\n","\n","print(\"Average cosine similarity of target & context embeddings of each word in...\")\n","print(\"Galvan Min Loss: \", mean(galvan_loss_ct))\n","print(\"Galvan Max Score: \", mean(galvan_score_ct))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFCn4qSowL0b","executionInfo":{"status":"ok","timestamp":1685450416126,"user_tz":-120,"elapsed":1046,"user":{"displayName":"Onur Akman","userId":"10086580318229224401"}},"outputId":"311e891e-5f7f-462d-ede4-d458e4608c94"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Average cosine similarity of target & context embeddings of each word in...\n","Galvan Min Loss:  4.941992656110668e-05\n","Galvan Max Score:  4.941992656110668e-05\n"]}]}]}